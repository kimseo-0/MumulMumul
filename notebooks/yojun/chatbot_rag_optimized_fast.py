# chatbot_rag_optimized.py
# Hybrid Cache + Metadata Packing + Structured Templates + ìµœì í™”ëœ RAG ë²„ì „

import os
import time
import numpy as np
from dotenv import load_dotenv
from operator import itemgetter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ==============================================================  
# ê¸°ë³¸ ì„¤ì •  
# ==============================================================  

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

SEARCH_K = 3
FETCH_K = 8

RAG_CHAIN = None


# ==============================================================
# Hybrid Cache (Exact + Semantic)
# ==============================================================

embedder_for_cache = OpenAIEmbeddings(model=EMBEDDING_MODEL)

CACHE = {
    "ì´ˆê¸‰": {"exact": {}, "semantic": []},
    "ì¤‘ê¸‰": {"exact": {}, "semantic": []},
    "ê³ ê¸‰": {"exact": {}, "semantic": []},
}

def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def search_cache(question: str, grade: str):
    """
    1) Exact cache
    2) Semantic cache
    """
    # Exact match
    if question in CACHE[grade]["exact"]:
        print("[CACHE HIT] Exact match")
        return CACHE[grade]["exact"][question]

    # Semantic match
    print("[CACHE CHECK] Semantic similarity...")
    q_vec = embedder_for_cache.embed_query(question)

    best_score = 0
    best_answer = None

    for entry in CACHE[grade]["semantic"]:
        score = cosine_similarity(q_vec, entry["vector"])
        if score > best_score:
            best_score = score
            best_answer = entry["answer"]

    if best_score >= 0.80:
        print(f"[CACHE HIT] Semantic score={best_score:.3f}")
        return best_answer

    return None  # ìºì‹œ MISS


def save_to_cache(question: str, grade: str, answer: str):
    """
    ìµœì¢… ë‹µë³€ ì „ì²´(answer) + ì¶œì²˜ í¬í•¨ ê·¸ëŒ€ë¡œ ì €ì¥
    """
    CACHE[grade]["exact"][question] = answer

    vec = embedder_for_cache.embed_query(question)
    CACHE[grade]["semantic"].append({
        "question": question,
        "vector": vec,
        "answer": answer
    })

    print("[CACHE SAVE] ì €ì¥ ì™„ë£Œ (exact + semantic)")


# ==============================================================  
# êµ¬ì¡°í™” í…œí”Œë¦¿ (ì´ˆê¸‰ / ì¤‘ê¸‰ / ê³ ê¸‰)  
# ==============================================================  

# ì—¬ê¸°ëŠ” ìºì‹œ ì½”ë“œ/í¬ë§· ì½”ë“œ ìœ„ì— ìˆì„ ìˆ˜ë„ ìˆê³  ì•„ë˜ì— ìˆì„ ìˆ˜ë„ ìˆìŒ
GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì‰¬ìš´ ë§ë¡œ í•œ ì¤„ë¡œ ë‹¤ì‹œ ì •ë¦¬í•©ë‹ˆë‹¤.

[í•µì‹¬ í•œ ì¤„ ìš”ì•½]
- ì´ ê°œë…ì„ ì´ˆë³´ìê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

[ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´, ì˜ì–´, ì¶•ì•½ì–´ëŠ” ìµœëŒ€í•œ í”¼í•˜ê³ 
  í•„ìš”í•  ë•ŒëŠ” ê´„í˜¸ë¡œ ì¦‰ì‹œ í’€ì´í•©ë‹ˆë‹¤.
  ì˜ˆ: â€œë¼ì´ë¸ŒëŸ¬ë¦¬(ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ê¸°ëŠ¥ ë¬¶ìŒ)â€
- ì„¤ëª…ì€ 2~4ì¤„ë¡œ ê°„ë‹¨í•˜ê³  ì§ê´€ì ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

[ë¹„ìœ  + ì˜ˆì‹œ ì½”ë“œ]
- í˜„ì‹¤ ì„¸ê³„ì˜ ë¹„ìœ ë¥¼ 1~2ê°œ ì œê³µí•©ë‹ˆë‹¤.
  ì¡°ê±´:
    1) ìƒí™œ ì† ë¬¼ê±´, ìŒì‹, ê¸°ì¡´ ê²½í—˜ ë“± ì´ˆë³´ìê°€ ë°”ë¡œ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ëŒ€ìƒ
    2) ë¹„ìœ ê°€ ê°œë…ê³¼ 1:1ë¡œ ì—°ê²°ë˜ë„ë¡ ì„¤ëª…

- ë¹„ìœ  â†’ ì½”ë“œ ê°œë… ì—°ê²° ê³¼ì •ì„ ëª…í™•íˆ ì ìŠµë‹ˆë‹¤.
  ì˜ˆ: â€œìš”ë¦¬ ë ˆì‹œí”¼ì— ì¬ë£Œë¥¼ ë„£ìœ¼ë©´ ê²°ê³¼ê°€ ë‚˜ì˜¤ë“¯,
       í•¨ìˆ˜ì— ì…ë ¥ì„ ë„£ìœ¼ë©´ ì¶œë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.â€

- ì´ˆë³´ìê°€ ë”°ë¼ ì¹  ìˆ˜ ìˆëŠ” ì§§ì€ ì½”ë“œ ì˜ˆì‹œ(3~7ì¤„)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

[ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ì ]
- ë¶€ë‹´ë˜ì§€ ì•Šì„ ì •ë„ì˜ ì¶”ê°€ ì„¤ëª… 1~2ì¤„

[ì—°ìŠµ ë¬¸ì œ]
- ì´ˆë³´ìê°€ í’€ ìˆ˜ ìˆëŠ” ì§§ì€ ì—°ìŠµ ë¬¸ì œ 1~2ê°œ ì œì‹œ
- ê° ë¬¸ì œì— 1ì¤„ íŒíŠ¸ ì œê³µ

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
  ì˜ˆ: â€œ01 íŒŒì´ì¬ ê¸°ì´ˆ ë¬¸ë²• I.pdf / p.3â€
""",

    "ì¤‘ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.

[ì •í™•í•œ ì •ì˜]
- context ê¸°ë°˜ìœ¼ë¡œ 2~4ì¤„ ì•ˆì—ì„œ ì •ì˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•„ìš” ì‹œ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥ (ë¶ˆí•„ìš”í•˜ê²Œ í™•ì¥ ê¸ˆì§€)

[ì™œ í•„ìš”í•œê°€]
- ì´ ê°œë…ì´ ì™œ ì¤‘ìš”í•œì§€ ì‹¤ë¬´ ë˜ëŠ” í•™ìŠµ ê´€ì ì—ì„œ 1~2ì¤„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

[ì‹¤ë¬´ ì£¼ì˜ í¬ì¸íŠ¸]
- ì‹¤ë¬´ ë˜ëŠ” í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì‹¤ìˆ˜í•˜ëŠ” ë¶€ë¶„, í—·ê°ˆë¦¬ëŠ” í¬ì¸íŠ¸ 1~3ê°œ ì œì‹œ

[ì˜ˆì‹œ ì½”ë“œ ë˜ëŠ” ê°„ë‹¨ ì˜ˆì œ]
- ì¤‘ê¸‰ì ìˆ˜ì¤€ì˜ ì½”ë“œ ì˜ˆì‹œë¥¼ 3~8ì¤„ ì œê³µ

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
""",

    "ê³ ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì˜ ë³¸ì§ˆì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

[ë‚´ë¶€ ë™ì‘ ì›ë¦¬]
- êµ¬ì¡°, ë©”ì»¤ë‹ˆì¦˜, íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ì›ë¦¬ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
- context ê¸°ë°˜ìœ¼ë¡œ ì„œìˆ í•˜ë©° ë¶ˆí•„ìš”í•œ ì™¸ë¶€ ì§€ì‹ í™•ì¥ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.

[ì„±ëŠ¥/ë©”ëª¨ë¦¬/íš¨ìœ¨ì„± ê´€ì ]
- ê°€ëŠ¥í•œ ê²½ìš° ì‹œê°„ ë³µì¡ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©, ì²˜ë¦¬ êµ¬ì¡° ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- contextì— ì¡´ì¬í•˜ëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

[ë¹„êµ]
- ìœ ì‚¬ ê°œë… ë˜ëŠ” ëŒ€ì•ˆ ê¸°ìˆ ê³¼ì˜ ì°¨ì´ë¥¼ 1~3ê°œ bulletë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

[ì˜ˆì‹œ ë˜ëŠ” ì ìš© ì‚¬ë¡€]
- ê³ ê¸‰ìì—ê²Œ ì í•©í•œ ì˜ˆì‹œ ë˜ëŠ” ê¸°ìˆ  ì ìš© ì‚¬ë¡€ë¥¼ 3~8ì¤„ ì‚¬ì´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
"""
}



# ==============================================================  
# metadata â†’ í…ìŠ¤íŠ¸ë¡œ íŒ¨í‚¹  
# ==============================================================  

def format_docs_with_metadata(docs):
    parts = []
    for idx, doc in enumerate(docs, start=1):
        meta = doc.metadata or {}

        file_name = meta.get("file_name") or meta.get("source") or "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"
        page = meta.get("page") or meta.get("page_number") or meta.get("page_index")

        if page:
            header = f"[{idx}] ì¶œì²˜: {file_name} / p.{page}"
        else:
            header = f"[{idx}] ì¶œì²˜: {file_name}"

        parts.append(f"{header}\n{doc.page_content}")
    return "\n\n".join(parts)


# ==============================================================  
# RAG ì²´ì¸ ì´ˆê¸°í™”  
# ==============================================================  

def initialize_rag_chain():
    print("[LOG] RAG ì²´ì¸ ì´ˆê¸°í™” ì‹œì‘")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION,
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ [Context] ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ê³ ,
ì¶œì²˜(íŒŒì¼ëª…, í˜ì´ì§€)ë¥¼ ë‹µë³€ ëì— í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ë‹µë³€ ê·œì¹™]
{grade_rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    chain = (
        {
            "docs": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "grade": itemgetter("grade"),
            "grade_rules": itemgetter("grade_rules"),
            "history": itemgetter("history"),
        }
        | RunnableLambda(lambda x: {**x, "context": format_docs_with_metadata(x["docs"])})
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ: {time.time() - start:.3f}ì´ˆ")
    return chain


def get_rag_chain():
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================  
# HISTORY (ë©€í‹°í„´)  
# ==============================================================  

def build_history_text(history, max_turns=2):
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join([f"í•™ìƒ: {h['question']}\nAI: {h['answer']}\n" for h in recent])


# ==============================================================  
# ì§ˆë¬¸ ë¶„ë¦¬  
# ==============================================================  

def split_questions(user_message: str):
    splitter = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template("""
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.

    1. ì§ˆë¬¸1
    2. ì§ˆë¬¸2

    ì˜ˆë¥¼ë“¤ì–´ í•œ ì§ˆë¬¸ì— ë¦¬ìŠ¤íŠ¸ì™€ ragì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì§ˆë¬¸ì´ ë‚˜ì™”ì„ ë•Œ ë‘ê°€ì§€ë¥¼ ë‚˜ëˆ ì„œ ì„¤ëª…í•˜ë¼ëŠ” ê²ƒ
    ë‹¨ì–´ë³„ë¡œ ìª¼ê°œì§€ë§ê²ƒ
                                              

    ë„ˆë¬´ ì˜ê²Œ ìª¼ê°œì§€ ë§ê³ , ì˜ë¯¸ ë‹¨ìœ„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ëˆ„ì„¸ìš”.

    ì‚¬ìš©ì ì…ë ¥:
    {message}
    """)

    raw = (prompt | splitter | StrOutputParser()).invoke({"message": user_message})
    questions = []

    for line in raw.splitlines():
        line = line.strip()
        if line and line[0].isdigit() and "." in line:
            _, q = line.split(".", 1)
            questions.append(q.strip())

    return questions or [user_message]


# ==============================================================  
# ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (ìºì‹œ ì ìš©ë¨)  
# ==============================================================  

def answer_single(question: str, grade: str, history: list):

    # 1) ìºì‹œ ë¨¼ì € í™•ì¸
    cached = search_cache(question, grade)
    if cached:
        print("[INFO] ìºì‹œì—ì„œ ì¦‰ì‹œ ë°˜í™˜")
        return cached

    # 2) RAG ì‹¤í–‰
    rag = get_rag_chain()
    history_text = build_history_text(history)

    start = time.time()
    answer = rag.invoke({
        "question": question,
        "grade": grade,
        "grade_rules": GRADE_RULES[grade],
        "history": history_text,
    })
    print(f"[Time] LLM ë‹µë³€ ìƒì„±: {time.time() - start:.3f}ì´ˆ")

    # 3) ìºì‹œì— ì €ì¥
    save_to_cache(question, grade, answer)

    # 4) history ì €ì¥
    history.append({"question": question, "answer": answer})

    return answer


# ==============================================================  
# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬  
# ==============================================================  

def multi_answer(user_message: str, grade: str, history: list):
    questions = split_questions(user_message)

    if len(questions) == 1:
        return answer_single(questions[0], grade, history)

    blocks = []
    for idx, q in enumerate(questions, start=1):
        ans = answer_single(q, grade, history)
        blocks.append(f"### ì§ˆë¬¸ {idx}\n> {q}\n\n{ans}\n---\n")

    return "\n".join(blocks)


# ==============================================================  
# CLI í…ŒìŠ¤íŠ¸  
# ==============================================================  

if __name__ == "__main__":
    print("\n=== Hybrid Cache + Metadata + Template RAG ì±—ë´‡ ===")

    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ").strip()
        if msg.lower() == "exit":
            break

        grade = input("ğŸ’¡ ë‚œì´ë„ ì„ íƒ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘...\n")
        result = multi_answer(msg, grade, history)

        print("\nğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ë‹µë³€:\n")
        print(result)
        print("\n============================================\n")
