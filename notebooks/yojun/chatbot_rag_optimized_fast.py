# ==============================================================
# chatbot_rag_final.py
# ì„¤ëª… ëª¨ë“œ + í•™ìŠµí€´ì¦ˆ ëª¨ë“œ ìë™ ë¶„ê¸°
# Hybrid Cache + Metadata Packing + Context í™•ì¥ + ë©€í‹°í„´ íˆìŠ¤í† ë¦¬
# ==============================================================
import os
import time
import numpy as np
from dotenv import load_dotenv
from operator import itemgetter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda


load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

SEARCH_K = 3
FETCH_K = 8

RAG_CHAIN = None
RETRIEVER = None


# ==============================================================
# Hybrid Cache (Exact + Semantic)
# ==============================================================

embedder_for_cache = OpenAIEmbeddings(model=EMBEDDING_MODEL)

CACHE = {
    "ì´ˆê¸‰": {"exact": {}, "semantic": []},
    "ì¤‘ê¸‰": {"exact": {}, "semantic": []},
    "ê³ ê¸‰": {"exact": {}, "semantic": []},
}

def cosine_similarity(a, b):
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def search_cache(question: str, grade: str):
    """ìºì‹œ ì¡°íšŒ"""
    if question in CACHE[grade]["exact"]:
        print("[CACHE HIT] Exact")
        return CACHE[grade]["exact"][question]

    print("[CACHE CHECK] Semantic...")
    q_vec = embedder_for_cache.embed_query(question)

    best_score, best_answer = 0, None
    for entry in CACHE[grade]["semantic"]:
        score = cosine_similarity(q_vec, entry["vector"])
        if score > best_score:
            best_score, best_answer = score, entry["answer"]

    if best_score >= 0.80:
        print(f"[CACHE HIT] Semantic score={best_score:.3f}")
        return best_answer

    return None

def save_to_cache(question: str, grade: str, answer: str):
    """ìºì‹œ ì €ì¥"""
    vec = embedder_for_cache.embed_query(question)
    CACHE[grade]["exact"][question] = answer

    CACHE[grade]["semantic"].append({
        "question": question,
        "vector": vec,
        "answer": answer
    })

    print("[CACHE SAVE] ì™„ë£Œ")


# ==============================================================
# "ì„¤ëª… ëª¨ë“œ" í…œí”Œë¦¿
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì‰¬ìš´ ë§ë¡œ í•œ ì¤„ ì •ë¦¬.

[í•µì‹¬ ìš”ì•½]
- ì´ˆë³´ìë„ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´ ìµœì†Œí™”. ì˜ì–´/ì¶•ì•½ì–´ ì¦‰ì‹œ í’€ì´.
- 2~4ì¤„ ì„¤ëª….

[ë¹„ìœ  + ì˜ˆì‹œ ì½”ë“œ]
- í˜„ì‹¤ ë¹„ìœ  ì œê³µ (1~2ê°œ)
- 3~7ì¤„ ê°„ë‹¨í•œ ì½”ë“œ ì˜ˆì‹œ í¬í•¨.

[ì¶”ê°€ ì„¤ëª…]
- 1~2ì¤„.

[ì—°ìŠµ ë¬¸ì œ]
- 1~2ê°œ + íŒíŠ¸ í¬í•¨.

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ ëª…í™•íˆ í‘œì‹œ.
""",

    "ì¤‘ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬.

[ì •í™•í•œ ì •ì˜]
- context ê¸°ë°˜ ì •ì˜ë¥¼ ì •í™•íˆ 2~4ì¤„ ì‘ì„±.

[ì™œ í•„ìš”í•œê°€]
- ì‹¤ë¬´/í•™ìŠµ ê´€ì  1~2ì¤„.

[ì£¼ì˜ í¬ì¸íŠ¸]
- í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„ 1~3ê°œ.

[ì˜ˆì‹œ ì½”ë“œ]
- 3~8ì¤„.

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ í¬í•¨.
""",

    "ê³ ê¸‰": """
[í•µì‹¬ ìš”ì•½]
- ê°œë…ì˜ ë³¸ì§ˆì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ë™ì‘ ì›ë¦¬]
- ë‚´ë¶€ êµ¬ì¡°/ë©”ì»¤ë‹ˆì¦˜ ì¤‘ì‹¬ ì„¤ëª….

[ì„±ëŠ¥/ë©”ëª¨ë¦¬]
- context ê¸°ë°˜ ë¶„ì„.

[ë¹„êµ]
- ìœ ì‚¬ ê¸°ìˆ  ë¹„êµ 1~3ê°œ bullet.

[ì‚¬ë¡€]
- 3~8ì¤„ë¡œ ê³ ê¸‰ ì˜ˆì‹œ.

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
"""
}


# ==============================================================
# í•™ìŠµí€´ì¦ˆ ìƒì„± ëª¨ë“œ í…œí”Œë¦¿
# ==============================================================

QUIZ_RULES_TEMPLATE = """
[ëª¨ë“œ]
- ì§€ê¸ˆ ìš”ì²­ì€ 'í•™ìŠµí€´ì¦ˆ ìƒì„±'ì…ë‹ˆë‹¤.
- ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[JSON ìŠ¤í‚¤ë§ˆ]
{
  "total": ë¬¸ì œìˆ˜,
  "items": [
    {
      "number": 1,
      "type": "ox" ë˜ëŠ” "multiple" ë˜ëŠ” "short",
      "question": "ë¬¸ì œ ë‚´ìš©",
      "choices": ["ë³´ê¸°1","ë³´ê¸°2"] ë˜ëŠ” null,
      "answer": "ì •ë‹µ",
      "difficulty": "ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰",
      "source_file": "íŒŒì¼ëª…",
      "source_page": 1
    }
  ]
}

[ë¬¸ì œ ìƒì„± ê·œì¹™]
- ë°˜ë“œì‹œ context ì•ˆì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ë¬¸ì œ ìƒì„±.
- typeì€ ox, short, multiple ì„ì–´ì„œ ìƒì„±.
- ë‚œì´ë„ëŠ” {GRADE_LEVEL} ë ˆë²¨ì— ë§ê²Œ.
- ì¶œì²˜ëŠ” context ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë„£ê¸°.

[ì¶œë ¥ ê·œì¹™]
- JSONë§Œ ì¶œë ¥.  
- ì ˆëŒ€ ì„¤ëª… ë¬¸ì¥ ì¶œë ¥ ê¸ˆì§€.
"""



# ==============================================================
# ë¬¸ì œ/í€´ì¦ˆ ìš”ì²­ì¸ì§€ íŒë‹¨
# ==============================================================

def is_quiz_request(q: str):
    q2 = q.replace(" ", "")
    keywords = ["í€´ì¦ˆ", "OX", "ë¬¸ì œ", "í…ŒìŠ¤íŠ¸", "ì—°ìŠµë¬¸ì œ", "5ë¬¸ì œ", "10ë¬¸ì œ"]
    return any(k.lower() in q2.lower() for k in keywords)


def build_rules(question: str, grade: str) -> str:
    if is_quiz_request(question):
        print("[MODE] í•™ìŠµí€´ì¦ˆ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        # format ëŒ€ì‹  ì•ˆì „í•˜ê²Œ replaceë§Œ ì‚¬ìš©
        return QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)
    else:
        return GRADE_RULES[grade]


# ==============================================================
# metadata â†’ í…ìŠ¤íŠ¸ íŒ¨í‚¹
# ==============================================================

def format_docs_with_metadata(docs):
    parts = []

    for idx, doc in enumerate(docs, start=1):
        meta = doc.metadata or {}

        file_name = (
            meta.get("filename")
            or meta.get("file_name")
            or meta.get("source")
            or meta.get("filename_eng")
            or "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"
        )

        page = meta.get("page") or meta.get("page_number") or meta.get("page_index")

        header = f"[{idx}] ì¶œì²˜: {file_name}"
        if page:
            header += f" / p.{page}"

        body = doc.page_content or ""

        parts.append(f"{header}\n{body}")

    return "\n\n".join(parts)



# ==============================================================
# ì§ˆë¬¸ ë‚œì´ë„ â†’ ê²€ìƒ‰ëŸ‰ ìë™ í™•ì¥ (Context í™•ì¥)
# ==============================================================

def estimate_topic_count(question: str) -> int:
    joiners = ["ì™€ ", "ê³¼ ", "ì´ë‘", "ë‘", " ë° ", " ê·¸ë¦¬ê³  ", ",", "/"]
    score = 1
    for j in joiners:
        if j in question:
            score += 1
    return max(1, min(score, 3))

def adjust_retriever_for_question(question: str):
    global RETRIEVER
    if RETRIEVER is None:
        return

    t = estimate_topic_count(question)

    if t == 1:
        k, f = 3, 8
    elif t == 2:
        k, f = 6, 16
    else:
        k, f = 9, 24

    RETRIEVER.search_kwargs["k"] = k
    RETRIEVER.search_kwargs["fetch_k"] = f

    print(f"[RETRIEVER] topic={t} â†’ k={k}, fetch_k={f}")


# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™”
# ==============================================================

def initialize_rag_chain():
    global RETRIEVER

    print("[LOG] RAG ì²´ì¸ ì´ˆê¸°í™”â€¦")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION
    )

    RETRIEVER = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ [Context] ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ê±°ë‚˜ ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ê·œì¹™]
{rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    chain = (
        {
            "docs": itemgetter("question") | RETRIEVER,
            "question": itemgetter("question"),
            "rules": itemgetter("rules"),
            "grade": itemgetter("grade"),
            "history": itemgetter("history"),
        }
        | RunnableLambda(lambda x: {
            **x,
            "context": format_docs_with_metadata(x["docs"])
        })
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] init ì™„ë£Œ: {time.time() - start:.2f}s")
    return chain


def get_rag_chain():
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN



# ==============================================================
# HISTORY (ë©€í‹°í„´)
# ==============================================================

def build_history_text(history, max_turns=2):
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join([f"í•™ìƒ: {h['question']}\nAI: {h['answer']}" for h in recent])



# ==============================================================
# ë©”ì¸ ë‹µë³€ í•¨ìˆ˜
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    """1ì§ˆë¬¸ â†’ 1ë‹µë³€"""

    cached = search_cache(question, grade)
    if cached:
        print("[INFO] ìºì‹œ ì‚¬ìš©")
        return cached

    # context í™•ì¥
    adjust_retriever_for_question(question)

    # ëª¨ë“œ ìë™ ê²°ì •
    rules_text = build_rules(question, grade)

    rag = get_rag_chain()
    history_text = build_history_text(history)

    start = time.time()
    answer = rag.invoke({
        "question": question,
        "grade": grade,
        "rules": rules_text,
        "history": history_text
    })
    print(f"[Time] ë‹µë³€ ìƒì„±: {time.time() - start:.3f}s")

    save_to_cache(question, grade, answer)

    history.append({"question": question, "answer": answer})
    return answer



# ==============================================================
# CLI ì‹¤í–‰ë¶€
# ==============================================================

if __name__ == "__main__":
    print("\n=== RAG ì±—ë´‡ (ì„¤ëª… + í•™ìŠµí€´ì¦ˆ 2ëª¨ë“œ ìë™ ë¶„ê¸°) ===\n")
    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ").strip()
        if msg.lower() == "exit":
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        grade = input("ğŸ’¡ ë‚œì´ë„ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        print("\nâ³ ìƒì„± ì¤‘...\n")
        result = answer_single(msg, grade, history)

        print("\nğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ì‘ë‹µ:\n")
        print(result)
        print("\n-----------------------------------\n")