# ==============================================================
# chatbot_rag_final.py
# ì„¤ëª… ëª¨ë“œ + í•™ìŠµí€´ì¦ˆ ëª¨ë“œ ìë™ ë¶„ê¸° (LLM ê¸°ë°˜ ì˜ë„ íŒë‹¨)
# ìºì‹œ + ë©”íƒ€ë°ì´í„° í¬ë§· + ë©€í‹°í„´ íˆìŠ¤í† ë¦¬
# ==============================================================

import os
import time
import numpy as np
from dotenv import load_dotenv
from operator import itemgetter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

# ë²¡í„°DB(Chroma)ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

# ê¸°ë³¸ ê²€ìƒ‰ ê°œìˆ˜ ì„¤ì •(ì§ˆë¬¸ ë‚´ìš©ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì¡°ì •ë¨)
SEARCH_K = 3
FETCH_K = 8

RAG_CHAIN = None   # ì‹¤ì œë¡œ ë‹µë³€ì„ ë§Œë“œëŠ” ì²´ì¸(íŒŒì´í”„ë¼ì¸)
RETRIEVER = None   # ë²¡í„°DBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì•„ì˜¤ëŠ” ì¹œêµ¬


# ==============================================================
# ìºì‹œ (ì´ë¯¸ í–ˆë˜ ì§ˆë¬¸/ë‹µì„ ê¸°ì–µí•´ë‘ê¸°)
# ==============================================================

# "semantic" ì´ë¼ëŠ” ë§ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´ì„œ ì‰½ê²Œ ì„¤ëª…:
# - ìš°ë¦¬ê°€ ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì„œ(ë²¡í„°) "ëœ»ì´ ë¹„ìŠ·í•œ ë¬¸ì¥"ì„ ì°¾ëŠ” ë°©ì‹ì„
#   ë³´í†µ "ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(semantic search)" ë¼ê³  ë¶€ë¦„.
# - ì—¬ê¸°ì„œëŠ” "ë¹„ìŠ·í•œ ì§ˆë¬¸"ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©í•¨.

embedder_for_cache = OpenAIEmbeddings(model=EMBEDDING_MODEL)

# ë‚œì´ë„ë³„ë¡œ ìºì‹œë¥¼ ë”°ë¡œ ê´€ë¦¬
# - exact: ì§ˆë¬¸ ë¬¸ì¥ì´ ì™„ì „íˆ ë˜‘ê°™ì„ ë•Œ
# - semantic: ì§ˆë¬¸ ë¬¸ì¥ì€ ë‹¤ë¥´ì§€ë§Œ 'ëœ»'ì´ ë¹„ìŠ·í•  ë•Œ
CACHE = {
    "ì´ˆê¸‰": {"exact": {}, "semantic": []},
    "ì¤‘ê¸‰": {"exact": {}, "semantic": []},
    "ê³ ê¸‰": {"exact": {}, "semantic": []},
}


def cosine_similarity(a, b):
    """ë‘ ë²¡í„°(ìˆ«ì ë¦¬ìŠ¤íŠ¸)ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ 0~1 ì‚¬ì´ ì ìˆ˜ë¡œ ê³„ì‚°"""
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def search_cache(question: str, grade: str, intent: str):
    """
    ìºì‹œì—ì„œ ë¨¼ì € ë‹µì„ ì°¾ì•„ë³´ëŠ” í•¨ìˆ˜.
    intent(ì„¤ëª… / í€´ì¦ˆ / ë¬´ê´€í•¨)ì— ë”°ë¼ ìºì‹œ ì‚¬ìš© ë°©ì‹ì„ ë‹¤ë¥´ê²Œ í•œë‹¤.

    1) exact ìºì‹œëŠ” í•­ìƒ ì‚¬ìš© (ë¬¸ì¥ì´ ì™„ì „íˆ ê°™ì„ ë•Œ)
    2) intent == "í€´ì¦ˆ"  â†’ semantic ìºì‹œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    3) intent == "ì„¤ëª…"  â†’ semantic ìºì‹œë„ ì‚¬ìš© (ëœ»ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸ ì¬ì‚¬ìš©)
    4) intent == "ë¬´ê´€í•¨" â†’ ì›ì¹™ì ìœ¼ë¡œ ìºì‹œë¥¼ ì“°ì§€ ì•Šë„ë¡ ìœ„ìª½ì—ì„œ ê±¸ëŸ¬ì§
    """
    # 1) ì™„ì „íˆ ê°™ì€ ì§ˆë¬¸ì¸ ê²½ìš° (ë¬¸ì¥ ê·¸ëŒ€ë¡œ ì¼ì¹˜)
    if question in CACHE[grade]["exact"]:
        print("[CACHE HIT] Exact")
        return CACHE[grade]["exact"][question]

    # 2) í€´ì¦ˆ ëª¨ë“œì—ì„œëŠ” semantic ìºì‹œ ì‚¬ìš© ê¸ˆì§€
    if intent == "í€´ì¦ˆ":
        print("[CACHE SKIP] í€´ì¦ˆ ëª¨ë“œ â†’ semantic ìºì‹œ ì‚¬ìš© ì•ˆ í•¨")
        return None

    # 3) ì„¤ëª… ëª¨ë“œì¼ ë•Œë§Œ semantic ìºì‹œ ì‚¬ìš©
    if intent != "ì„¤ëª…":
        # ì•ˆì „ì¥ì¹˜: í˜¹ì‹œ ëª¨ë¥´ëŠ” intent ê°’ì´ë©´ semantic ìºì‹œë¥¼ ì“°ì§€ ì•ŠìŒ
        print("[CACHE SKIP] ì„¤ëª…/í€´ì¦ˆ ëª¨ë“œê°€ ì•„ë‹ˆë¯€ë¡œ semantic ìºì‹œ ì‚¬ìš© ì•ˆ í•¨")
        return None

    print("[CACHE CHECK] Semantic...")
    q_vec = embedder_for_cache.embed_query(question)

    best_score = 0.0
    best_answer = None

    for entry in CACHE[grade]["semantic"]:
        score = cosine_similarity(q_vec, entry["vector"])
        if score > best_score:
            best_score = score
            best_answer = entry["answer"]

    # 0.8 ì´ìƒì´ë©´ "ê½¤ ë¹„ìŠ·í•˜ë‹¤"ë¼ê³  ë³´ê³  ì¬ì‚¬ìš©
    if best_score >= 0.80:
        print(f"[CACHE HIT] Semantic score={best_score:.3f}")
        return best_answer

    return None




def save_to_cache(question: str, grade: str, answer: str):
    """ìƒˆë¡œ ë§Œë“  ë‹µë³€ì„ ìºì‹œì— ì €ì¥"""
    vec = embedder_for_cache.embed_query(question)

    # 1) ì™„ì „íˆ ê°™ì€ ì§ˆë¬¸ìš© ìºì‹œ
    CACHE[grade]["exact"][question] = answer

    # 2) 'ëœ»ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸' ê²€ìƒ‰ìš© ìºì‹œ
    CACHE[grade]["semantic"].append({
        "question": question,
        "vector": vec,
        "answer": answer
    })

    print("[CACHE SAVE] ì™„ë£Œ")


# ==============================================================
# ì„¤ëª… ëª¨ë“œ í…œí”Œë¦¿
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì‰¬ìš´ ë§ë¡œ í•œ ì¤„ ì •ë¦¬.

[í•µì‹¬ ìš”ì•½]
- ì´ˆë³´ìë„ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´ ìµœì†Œí™”. ì¦‰ì‹œ í’€ì´ í¬í•¨.
- 2~4ì¤„ë¡œ ì„¤ëª….

[ë¹„ìœ  + ì˜ˆì‹œ ì½”ë“œ]
- í˜„ì‹¤ ë¹„ìœ  1~2ê°œ.
- ì½”ë“œ ì˜ˆì‹œëŠ” 3~7ì¤„, ë°˜ë“œì‹œ ```python ì½”ë“œë¸”ë¡```ìœ¼ë¡œ ì‘ì„±.

[ì¶”ê°€ ì„¤ëª…]
- 1~2ì¤„.

[ì—°ìŠµ ë¬¸ì œ]
- 1~2ê°œ, íŒíŠ¸ í¬í•¨. ë°˜ë“œì‹œ context ì•ˆì˜ ë‚´ìš© ê¸°ë°˜.

[ì¶œì²˜]
- ì‚¬ìš©í•œ ë‚´ìš©ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
""",

    "ì¤‘ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬.

[ì •í™•í•œ ì •ì˜]
- context ê¸°ë°˜ìœ¼ë¡œ 2~4ì¤„.

[ì™œ í•„ìš”í•œê°€]
- ì‹¤ë¬´/í•™ìŠµ ê´€ì ìœ¼ë¡œ 1~2ì¤„.

[ì£¼ì˜ í¬ì¸íŠ¸]
- í—·ê°ˆë¦¬ê¸° ì‰¬ìš´ ë¶€ë¶„ 1~3ê°œ bullet.

[ì˜ˆì‹œ ì½”ë“œ]
- 3~8ì¤„, ë°˜ë“œì‹œ ```python ì½”ë“œë¸”ë¡``` ì‚¬ìš©.

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
""",

    "ê³ ê¸‰": """
[í•µì‹¬ ìš”ì•½]
- ê°œë…ì˜ ë³¸ì§ˆì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ë™ì‘ ì›ë¦¬]
- ë‚´ë¶€ êµ¬ì¡°/ë©”ì»¤ë‹ˆì¦˜ ì¤‘ì‹¬ ì„¤ëª….

[ì„±ëŠ¥/ë©”ëª¨ë¦¬]
- context ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª….

[ë¹„êµ]
- ìœ ì‚¬ ê¸°ìˆ ê³¼ì˜ ë¹„êµ 1~3ê°œ.

[ì‚¬ë¡€]
- 3~8ì¤„ì˜ ê³ ê¸‰ ì˜ˆì‹œ ì½”ë“œ(ì½”ë“œë¸”ë¡).

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
"""
}


# ==============================================================
# í•™ìŠµí€´ì¦ˆ ìƒì„± ëª¨ë“œ í…œí”Œë¦¿
# ==============================================================

QUIZ_RULES_TEMPLATE = """
[ëª¨ë“œ]
- ì´ ìš”ì²­ì€ 'í•™ìŠµí€´ì¦ˆ ìƒì„±'ì…ë‹ˆë‹¤.
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ì •ë‹µì€ context(ê°•ì˜ìë£Œ)ì— ê·¼ê±°í•´ì•¼ í•˜ì§€ë§Œ,
  í•™ìŠµ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë²”ìœ„ ë‚´ì—ì„œ ì•½ê°„ì˜ ì¼ë°˜ íŒŒì´ì¬ ì§€ì‹ í™•ì¥ì€ í—ˆìš©í•©ë‹ˆë‹¤.

[í—ˆìš© ë¬¸ì œ í˜•ì‹]
1) OX ë¬¸ì œ
2) ê°ê´€ì‹ ë¬¸ì œ(ë³´ê¸° 5ê°œ)

[ë¬¸ì œ ê°œìˆ˜ ê·œì¹™]
- ì‚¬ìš©ìê°€ ë¬¸ì œ ê°œìˆ˜ë¥¼ ì§ì ‘ ìš”ì²­í–ˆë‹¤ë©´, ìš”ì²­í•œ ê°œìˆ˜ë§Œ ìƒì„±í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë¬¸ì œ ê°œìˆ˜ë¥¼ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ 5ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

[ë¬¸ì œ ìœ í˜• ê¸°ë³¸ ê·œì¹™]
- ì‚¬ìš©ìê°€ ë¬¸ì œ ìœ í˜•ì„ íŠ¹ë³„íˆ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ ê°ê´€ì‹(multiple) ë¬¸ì œ ìœ„ì£¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
- í•„ìš”í•œ ê²½ìš° OX ë¬¸ì œë¥¼ ì„ì–´ë„ ë˜ì§€ë§Œ, ë‚œì´ë„ì™€ í•™ìŠµ íš¨ê³¼ë¥¼ ê³ ë ¤í•´ì„œ ì ì ˆíˆ ì„ íƒí•©ë‹ˆë‹¤.

[ì½”ë“œê°€ ë“¤ì–´ê°€ëŠ” ë¬¸ì œ ê·œì¹™]
- ì½”ë“œê°€ í¬í•¨ëœ ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì½”ë“œë¸”ë¡( ```python ... ``` ) í˜•íƒœë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
- ì½”ë“œë¸”ë¡ ë‚´ë¶€ëŠ” ë“¤ì—¬ì“°ê¸°ì™€ ì¤„ë°”ê¿ˆì„ ì •í™•í•˜ê²Œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

[JSON ìŠ¤í‚¤ë§ˆ]
{
  "total": ë¬¸ì œìˆ˜,
  "items": [
    {
      "number": 1,
      "type": "ox" ë˜ëŠ” "multiple",
      "question": "ë¬¸ì œ ë‚´ìš© (í•„ìš”ì‹œ ì½”ë“œë¸”ë¡ í¬í•¨)",
      "choices": ["ë³´ê¸°1","ë³´ê¸°2","ë³´ê¸°3","ë³´ê¸°4","ë³´ê¸°5"] ë˜ëŠ” null,
      "answer": "ì •ë‹µ(ë˜ëŠ” ì •ë‹µ ë³´ê¸° ë‚´ìš©)",
      "difficulty": "{GRADE_LEVEL}",
      "source_file": "íŒŒì¼ëª…",
      "source_page": í˜ì´ì§€ë²ˆí˜¸
    }
  ]
}

[ë¬¸ì œ ìƒì„± ê·œì¹™]
- context ì•ˆì˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ ì¶œì œí•©ë‹ˆë‹¤.
- ë‹¨, ìˆ˜ê°•ìƒ í•™ìŠµì„ ë•ëŠ” ë²”ìœ„ ë‚´ì—ì„œëŠ” ì•½ê°„ì˜ í™•ì¥ë„ í—ˆìš©í•©ë‹ˆë‹¤.
- ê°ê´€ì‹ ë³´ê¸°ëŠ” ë°˜ë“œì‹œ 5ê°œì—¬ì•¼ í•©ë‹ˆë‹¤.
- ê°ê´€ì‹ ë¬¸ì œì˜ ì •ë‹µì€ ë°˜ë“œì‹œ ë³´ê¸° ì•ˆì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë¬¸ì œëŠ” ìˆ˜ê°•ìƒì´ í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.

[ì¶œë ¥ ê·œì¹™]
- JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. (ì„¤ëª…, í•´ì„¤, ìì—°ì–´ ë¬¸ì¥ ì ˆëŒ€ ê¸ˆì§€)
"""


# ==============================================================
# ì§ˆë¬¸ ì˜ë„(ì„¤ëª…/í€´ì¦ˆ) LLMìœ¼ë¡œ íŒë‹¨
# ==============================================================

INTENT_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë„¤ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.

[ì„¤ëª…]
- ê°œë… ì„¤ëª…ë§Œ ìš”ì²­í•˜ëŠ” ê²½ìš°

[í€´ì¦ˆ]
- ë¬¸ì œ(í€´ì¦ˆ)ë§Œ ìš”ì²­í•˜ëŠ” ê²½ìš°

[í˜¼í•©]
- ì„¤ëª… ìš”ì²­ + ë¬¸ì œ ìš”ì²­ì´ í•¨ê»˜ í¬í•¨ëœ ê²½ìš°  
- ì•„ë˜ íŒ¨í„´ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ë°˜ë“œì‹œ 'í˜¼í•©'ìœ¼ë¡œ ë¶„ë¥˜:
  "ì„¤ëª…í•´ì£¼ê³  ë¬¸ì œ", "ì„¤ëª… + ë¬¸ì œ", "ì„¤ëª…í•˜ê³  OX", 
  "ì„¤ëª…í•´ì£¼ê³  ì—°ìŠµë¬¸ì œ", "ì•Œë ¤ì£¼ê³  ë¬¸ì œ", "ì„¤ëª… í›„ ë¬¸ì œ"

[ë¬´ê´€í•¨]
- í•™ìŠµê³¼ ê´€ê³„ì—†ëŠ” ì§ˆë¬¸

ì¶œë ¥ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë§Œ:
ì„¤ëª…
í€´ì¦ˆ
í˜¼í•©
ë¬´ê´€í•¨

ì‚¬ìš©ì ì§ˆë¬¸:
{question}
"""




def detect_intent(question: str, llm: ChatOpenAI) -> str:
    prompt = INTENT_PROMPT.format(question=question)
    result = llm.invoke(prompt)
    text = result.content.strip()

    if "ë¬´ê´€í•¨" in text:
        return "ë¬´ê´€í•¨"
    if "í˜¼í•©" in text:
        return "í˜¼í•©"
    if "í€´ì¦ˆ" in text:
        return "í€´ì¦ˆ"
    return "ì„¤ëª…"




def build_rules(question: str, grade: str, llm_for_intent: ChatOpenAI) -> str:
    intent = detect_intent(question, llm_for_intent)
    print(f"[INTENT] íŒë‹¨ ê²°ê³¼: {intent}")

    if intent == "ë¬´ê´€í•¨":
        return "IRRELEVANT"

    if intent == "í€´ì¦ˆ":
        print("[MODE] í•™ìŠµí€´ì¦ˆ ëª¨ë“œ (LLM íŒë‹¨)")
        return QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)

    if intent == "í˜¼í•©":
        print("[MODE] í˜¼í•© ëª¨ë“œ (LLM íŒë‹¨)")
        # ì„¤ëª… ê·œì¹™ + í€´ì¦ˆ ê·œì¹™ì„ í•©ì¹œë‹¤
        mixed = (
            "### ì„¤ëª… ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.\n" +
            GRADE_RULES[grade] +
            "\n\n### ê·¸ëŸ° ë‹¤ìŒ, ì•„ë˜ ê·œì¹™ì— ë”°ë¼ JSON í€´ì¦ˆë¥¼ ìƒì„±í•˜ì„¸ìš”.\n" +
            QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)
        )
        return mixed

    # ê·¸ ì™¸ëŠ” ì„¤ëª…
    print("[MODE] ì„¤ëª… ëª¨ë“œ (LLM íŒë‹¨)")
    return GRADE_RULES[grade]




# ==============================================================
# metadata â†’ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
# ==============================================================

def format_docs_with_metadata(docs):
    """
    ë²¡í„°DBì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(docs)ë¥¼
    - [1] ì¶œì²˜: íŒŒì¼ëª… / p.í˜ì´ì§€
    - ë³¸ë¬¸ ë‚´ìš©
    ì´ëŸ° í˜•íƒœì˜ í° ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜.
    """
    parts = []

    for idx, doc in enumerate(docs, start=1):
        meta = doc.metadata or {}

        file_name = (
            meta.get("filename")
            or meta.get("file_name")
            or meta.get("source")
            or meta.get("filename_eng")
            or "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"
        )

        page = meta.get("page") or meta.get("page_number") or meta.get("page_index")

        header = f"[{idx}] ì¶œì²˜: {file_name}"
        if page is not None:
            header += f" / p.{page}"

        body = doc.page_content or ""

        parts.append(f"{header}\n{body}")

    return "\n\n".join(parts)


# ==============================================================
# ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ê²€ìƒ‰ëŸ‰ ì¡°ì ˆ
# ==============================================================

def estimate_topic_count(question: str) -> int:
    """
    ì§ˆë¬¸ ì•ˆì— 'ê·¸ë¦¬ê³ , /, ì™€, ê³¼' ê°™ì€ ì—°ê²°ì–´ê°€ ëª‡ ê°œ ìˆëŠ”ì§€ ëŒ€ëµ ì„¸ì–´ì„œ
    - í•œ ê°€ì§€ ì£¼ì œì¸ì§€
    - ë‘ì„¸ ê°€ì§€ë¥¼ í•œ ë²ˆì— ë¬¼ì–´ë³´ëŠ”ì§€
    ë¥¼ ì•„ì£¼ ë‹¨ìˆœí•˜ê²Œ ì¶”ì •.
    """
    joiners = ["ì™€ ", "ê³¼ ", "ì´ë‘", "ë‘", " ë° ", " ê·¸ë¦¬ê³  ", ",", "/"]
    score = 1
    for j in joiners:
        if j in question:
            score += 1
    return max(1, min(score, 3))  # 1~3 ì‚¬ì´ë¡œ ì œí•œ


def adjust_retriever_for_question(question: str):
    """
    ì§ˆë¬¸ì´ ì—¬ëŸ¬ ì£¼ì œë¥¼ ì„ì–´ì„œ ë¬¼ì–´ë³´ëŠ” ê²ƒ ê°™ìœ¼ë©´
    í•œ ë²ˆì— ê°€ì ¸ì˜¤ëŠ” ë¬¸ì„œ ìˆ˜(k, fetch_k)ë¥¼ ëŠ˜ë ¤ì£¼ëŠ” í•¨ìˆ˜.
    """
    global RETRIEVER
    if RETRIEVER is None:
        return

    t = estimate_topic_count(question)

    if t == 1:
        k, f = 3, 8
    elif t == 2:
        k, f = 6, 16
    else:
        k, f = 9, 24

    RETRIEVER.search_kwargs["k"] = k
    RETRIEVER.search_kwargs["fetch_k"] = f

    print(f"[RETRIEVER] topic={t} â†’ k={k}, fetch_k={f}")


# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™”
# ==============================================================

def initialize_rag_chain():
    """
    - ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    - Chroma ë²¡í„°DB ì—°ê²°
    - Retriever ìƒì„±
    - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + LLM ì—°ê²°
    - ì „ì²´ íŒŒì´í”„ë¼ì¸(chain) êµ¬ì„±
    """
    global RETRIEVER

    print("[LOG] RAG ì²´ì¸ ì´ˆê¸°í™”â€¦")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION
    )

    RETRIEVER = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì„¤ëª… ëª¨ë“œì¼ ë•Œ: ë°˜ë“œì‹œ [Context] ì•ˆì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
- í€´ì¦ˆ ëª¨ë“œì¼ ë•Œ: ë¬¸ì œëŠ” contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ê³ ,
  í•„ìš”í•˜ë‹¤ë©´ í•™ìŠµ íš¨ê³¼ë¥¼ ìœ„í•´ ê°€ë³ê²Œ í™•ì¥í•  ìˆ˜ ìˆì§€ë§Œ,
  ì •ë‹µì€ context ì•ˆì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ê·œì¹™]
{rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    chain = (
        {
            "docs": itemgetter("question") | RETRIEVER,
            "question": itemgetter("question"),
            "rules": itemgetter("rules"),
            "grade": itemgetter("grade"),
            "history": itemgetter("history"),
        }
        # docsë¥¼ ë°›ì•„ì„œ context ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” ë‹¨ê³„
        | RunnableLambda(lambda x: {
            **x,
            "context": format_docs_with_metadata(x["docs"])
        })
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] init ì™„ë£Œ: {time.time() - start:.2f}s")
    return chain


def get_rag_chain():
    """RAG ì²´ì¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ì¬ì‚¬ìš©"""
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================
# HISTORY (ë©€í‹°í„´)
# ==============================================================

def build_history_text(history, max_turns=2):
    """
    ì´ì „ ëŒ€í™” ë‚´ìš© ë¦¬ìŠ¤íŠ¸(history)ë¥¼
    "í•™ìƒ: ... / AI: ..." í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜.
    ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìµœê·¼ max_turnsë²ˆë§Œ ì‚¬ìš©.
    """
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join(
        [f"í•™ìƒ: {h['question']}\nAI: {h['answer']}" for h in recent]
    )


# ==============================================================
# ë©”ì¸ ë‹µë³€ í•¨ìˆ˜
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    """
    ìƒˆ íë¦„:
    1) LLMìœ¼ë¡œ ì˜ë„ íŒë‹¨ (ì„¤ëª… / í€´ì¦ˆ / í˜¼í•© / ë¬´ê´€í•¨)
    2) ë¬´ê´€í•¨ì´ë©´ ë°”ë¡œ ì¢…ë£Œ
    3) intentì— ë”°ë¼ 1ë‹¨ê³„ ë˜ëŠ” 2ë‹¨ê³„ ì²˜ë¦¬
    """

    # 1) LLMìœ¼ë¡œ ì˜ë„ íŒë‹¨
    llm_for_intent = ChatOpenAI(model=LLM_MODEL, temperature=0.0)
    intent = detect_intent(question, llm_for_intent)
    print(f"[INTENT] íŒë‹¨ ê²°ê³¼: {intent}")

    # 2) ë¬´ê´€í•œ ì§ˆë¬¸
    if intent == "ë¬´ê´€í•¨":
        answer = "í•™ìŠµê³¼ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤."
        history.append({"question": question, "answer": answer})
        return answer

    # ==========================================================
    # 3) ì„¤ëª… ëª¨ë“œ â†’ ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ 1ë‹¨ê³„ ì‹¤í–‰
    # ==========================================================
    if intent == "ì„¤ëª…":
        rules_text = GRADE_RULES[grade]

        # ìºì‹œ í™•ì¸
        cached = search_cache(question, grade, intent)
        if cached:
            history.append({"question": question, "answer": cached})
            return cached

        adjust_retriever_for_question(question)
        rag = get_rag_chain()
        history_text = build_history_text(history)

        answer = rag.invoke({
            "question": question,
            "grade": grade,
            "rules": rules_text,
            "history": history_text
        })

        save_to_cache(question, grade, answer)
        history.append({"question": question, "answer": answer})
        return answer

    # ==========================================================
    # 4) í€´ì¦ˆ ëª¨ë“œ â†’ ê¸°ì¡´ì²˜ëŸ¼ JSONë§Œ ìƒì„±
    # ==========================================================
    if intent == "í€´ì¦ˆ":
        rules_text = QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)

        cached = search_cache(question, grade, intent)
        if cached:
            history.append({"question": question, "answer": cached})
            return cached

        adjust_retriever_for_question(question)
        rag = get_rag_chain()
        history_text = build_history_text(history)

        answer = rag.invoke({
            "question": question,
            "grade": grade,
            "rules": rules_text,
            "history": history_text
        })

        save_to_cache(question, grade, answer)
        history.append({"question": question, "answer": answer})
        return answer

    # ==========================================================
    # 5) ğŸŸ¡ í˜¼í•© ëª¨ë“œ (í•µì‹¬!)
    # ==========================================================
    if intent == "í˜¼í•©":
        print("[MODE] í˜¼í•© ëª¨ë“œ (2ë‹¨ê³„ ì²˜ë¦¬)")

        # ------------------------------------------------------
        # 5-1ë‹¨ê³„: ì„¤ëª… ë¨¼ì € ìƒì„±
        # ------------------------------------------------------
        explain_rules = GRADE_RULES[grade]

        adjust_retriever_for_question(question)
        rag = get_rag_chain()
        history_text = build_history_text(history)

        explanation = rag.invoke({
            "question": question,
            "grade": grade,
            "rules": explain_rules,
            "history": history_text
        })

        # ------------------------------------------------------
        # 5-2ë‹¨ê³„: í€´ì¦ˆë§Œ ë”°ë¡œ ìƒì„±(JSON)
        # ------------------------------------------------------
        quiz_rules = QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)

        quiz_json = rag.invoke({
            "question": question,
            "grade": grade,
            "rules": quiz_rules,
            "history": history_text
        })

        # ------------------------------------------------------
        # 5-3ë‹¨ê³„: ë‘˜ì„ í•©ì³ì„œ ìµœì¢… ê²°ê³¼ ë§Œë“¤ê¸°
        # ------------------------------------------------------
        final_output = (
            "### ğŸ“˜ ì„¤ëª…\n"
            + explanation.strip()
            + "\n\n---\n\n"
            + "### ğŸ“ í•™ìŠµ í€´ì¦ˆ(JSON)\n"
            + quiz_json.strip()
        )

        # í˜¼í•© ëª¨ë“œëŠ” ìºì‹œì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ë’¤ì„ì¸ ì§ˆë¬¸ íŠ¹ì„± ë•Œë¬¸ì—)
        history.append({"question": question, "answer": final_output})

        return final_output



# ==============================================================
# CLI ì‹¤í–‰ë¶€ (í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ìš©)
# ==============================================================
def ask_grade_level():
    """
    ì‚¬ìš©ìì—ê²Œ ë‚œì´ë„(ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰)ë¥¼ ì…ë ¥ë°›ë˜,
    ì˜¤íƒ€ë‚˜ ì˜ëª»ëœ ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ê³„ì† ë‹¤ì‹œ ì…ë ¥í•˜ê²Œ í•œë‹¤.
    """
    valid = {"ì´ˆê¸‰", "ì¤‘ê¸‰", "ê³ ê¸‰"}

    while True:
        grade = input("ğŸ’¡ ë‚œì´ë„ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        if grade in valid:
            return grade

        print("âš  ì…ë ¥í•œ ë‚œì´ë„ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n")

        
if __name__ == "__main__":
    print("\n=== RAG í•™ìŠµ ë„ìš°ë¯¸ ì±—ë´‡ (ì„¤ëª… + í•™ìŠµí€´ì¦ˆ ìë™ ë¶„ê¸°) ===\n")
    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ").strip()
        if msg.lower() == "exit":
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ì•ˆì „í•œ ë‚œì´ë„ ì…ë ¥
        grade = ask_grade_level()

        print("\nâ³ ìƒì„± ì¤‘...\n")
        result = answer_single(msg, grade, history)

        print("\nğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ì‘ë‹µ:\n")
        print(result)
        print("\n-----------------------------------\n")

