# chatbot_rag_optimized_fast.py
# "ìµœì  ì†ë„ ë²„ì „" - í”„ë¡¬í”„íŠ¸ ìµœì í™” + ëª¨ë¸ ë³€ê²½ + retriever ìµœì í™”

import os
import time
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì • (ì†ë„ ì¤‘ì‹¬ íŠœë‹)
# ==============================================================

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"   # â˜… ê¸°ì¡´ gpt-4o-mini â†’ ë¹ ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
EMBEDDING_MODEL = "text-embedding-3-large"  # â˜… ì†ë„ ì¤‘ì‹¬

SEARCH_K = 3       # â˜… ê¸°ì¡´ 5 â†’ 3
FETCH_K = 8        # â˜… ê¸°ì¡´ 20 â†’ 8

RAG_CHAIN = None   # ìºì‹±


# ==============================================================
# ìˆ˜ì¤€ë³„ ê·œì¹™ (ìµœì í™”ëœ ê°„ë‹¨ ë²„ì „)
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
- ì–´ë ¤ìš´ ìš©ì–´/ì˜ì–´ ìµœì†Œí™”
- ë¬¸ì¥ì€ ì§§ê²Œ
- í•„ìš”í•œ ê²½ìš° ê´„í˜¸ë¡œ í’€ì´
- ë‹µë³€ í˜•ì‹:
  1) ì§ˆë¬¸ ì´í•´(í•œì¤„)
  2) í•µì‹¬ ìš”ì•½(í•œì¤„)
  3) ì‰¬ìš´ ì„¤ëª…
  4) ë¹„ìœ  + ì§§ì€ ì˜ˆì‹œ ì½”ë“œ
  5) ì¶”ê°€ ì„¤ëª…(1~2ì¤„)
  6) ì¶œì²˜
""",
    "ì¤‘ê¸‰": """
- í•µì‹¬ ê°œë… ì •í™•íˆ
- ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥
- ì™œ í•„ìš”í•œì§€(1ì¤„)
- ì‹¤ë¬´ ì£¼ì˜ì  í¬í•¨
- ì¶œì²˜ í¬í•¨
""",
    "ê³ ê¸‰": """
- ë‚´ë¶€ ë™ì‘ ì›ë¦¬ ì¤‘ì‹¬
- êµ¬ì¡°/ì„±ëŠ¥/ë¹„êµ ì„¤ëª… ê°€ëŠ¥
- í•„ìš” ì‹œ ìˆ˜ì‹/ì „ë¬¸ìš©ì–´ ì‚¬ìš©
- ì¶œì²˜ í¬í•¨
"""
}

# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™” + ìºì‹±
# ==============================================================

def initialize_rag_chain():
    start = time.time()
    print("[LOG] ì´ˆê¸°í™” ì‹œì‘")

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION,
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ Context ì•ˆì˜ ì •ë³´ë§Œ ì´ìš©í•˜ì—¬ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
ì¶œì²˜(íŒŒì¼ëª…, í˜ì´ì§€)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ë‹µë³€ ê·œì¹™]
{grade_rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------

ê°€ëŠ¥í•œ í•œ ê°„ë‹¨í•˜ê³  ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    """

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    rag_chain = (
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "grade": itemgetter("grade"),
            "grade_rules": itemgetter("grade_rules"),
            "history": itemgetter("history"),
        }
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] RAG ì´ˆê¸°í™”: {time.time() - start:.3f}ì´ˆ")
    return rag_chain


def get_rag_chain():
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================
# HISTORY
# ==============================================================

def build_history_text(history, max_turns=2):
    """
    history ê¸¸ì´ë¥¼ 2í„´ë§Œ ë°˜ì˜ â†’ ì†ë„ ê°œì„ 
    """
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join([
        f"í•™ìƒ: {h['question']}\nAI: {h['answer']}\n"
        for h in recent
    ])


# ==============================================================
# ì§ˆë¬¸ ë¶„ë¦¬
# ==============================================================

def split_questions(user_message: str):
    splitter_model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template(
        """
ì‚¬ìš©ìì˜ ì…ë ¥ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ë¶„ë¦¬í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
1. ì§ˆë¬¸1
2. ì§ˆë¬¸2

ì‚¬ìš©ì ì…ë ¥:
{message}
"""
    )

    chain = prompt | splitter_model | StrOutputParser()
    raw = chain.invoke({"message": user_message})

    questions = []
    for line in raw.splitlines():
        line = line.strip()
        if line and line[0].isdigit() and "." in line:
            try:
                _, q = line.split(".", 1)
                questions.append(q.strip())
            except:
                pass

    if not questions:
        return [user_message]
    return questions


# ==============================================================
# ë‹¨ì¼ ì§ˆë¬¸ ë‹µë³€
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    rag = get_rag_chain()
    history_text = build_history_text(history)

    start = time.time()
    
    answer_text = rag.invoke({
        "question": question,
        "grade": grade,
        "grade_rules": GRADE_RULES[grade],
        "history": history_text,
    })

    print(f"[Time] LLM ë‹µë³€ ìƒì„±: {time.time() - start:.3f}ì´ˆ")

    history.append({"question": question, "answer": answer_text})
    return answer_text


# ==============================================================
# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
# ==============================================================

def multi_answer(user_message: str, grade: str, history: list):
    questions = split_questions(user_message)

    if len(questions) == 1:
        return answer_single(questions[0], grade, history)

    outputs = []
    for idx, q in enumerate(questions, start=1):
        ans = answer_single(q, grade, history)
        outputs.append(
            f"### ì§ˆë¬¸ {idx}\n> {q}\n\n{ans}\n\n---"
        )
    return "\n".join(outputs)


# ==============================================================
# CLI í…ŒìŠ¤íŠ¸
# ==============================================================

if __name__ == "__main__":
    print("\n=== ìµœì  ì†ë„ ë²„ì „ RAG ì±—ë´‡ ===\n")

    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥(exit ì¢…ë£Œ): ")
        if msg.lower() == "exit":
            break

        grade = input("ğŸ’¡ ë‚œì´ë„(ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘...\n")
        result = multi_answer(msg, grade, history)

        print("\nğŸ§  ë‹µë³€:\n", result)
        print("\n------------------------------------\n")
