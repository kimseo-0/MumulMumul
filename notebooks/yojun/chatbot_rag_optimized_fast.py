# ==============================================================
# chatbot_rag_final.py
# ì„¤ëª… ëª¨ë“œ + í•™ìŠµí€´ì¦ˆ ëª¨ë“œ ìë™ ë¶„ê¸° (LLM ê¸°ë°˜ ì˜ë„ íŒë‹¨)
# ìºì‹œ + ë©”íƒ€ë°ì´í„° í¬ë§· + ë©€í‹°í„´ íˆìŠ¤í† ë¦¬
# ==============================================================

import os
import time
import numpy as np
from dotenv import load_dotenv
from operator import itemgetter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

# ë²¡í„°DB(Chroma)ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

# ê¸°ë³¸ ê²€ìƒ‰ ê°œìˆ˜ ì„¤ì •(ì§ˆë¬¸ ë‚´ìš©ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì¡°ì •ë¨)
SEARCH_K = 3
FETCH_K = 8

RAG_CHAIN = None   # ì‹¤ì œë¡œ ë‹µë³€ì„ ë§Œë“œëŠ” ì²´ì¸(íŒŒì´í”„ë¼ì¸)
RETRIEVER = None   # ë²¡í„°DBì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì•„ì˜¤ëŠ” ì¹œêµ¬


# ==============================================================
# ìºì‹œ (ì´ë¯¸ í–ˆë˜ ì§ˆë¬¸/ë‹µì„ ê¸°ì–µí•´ë‘ê¸°)
# ==============================================================

# "semantic" ì´ë¼ëŠ” ë§ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´ì„œ ì‰½ê²Œ ì„¤ëª…:
# - ìš°ë¦¬ê°€ ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì„œ(ë²¡í„°) "ëœ»ì´ ë¹„ìŠ·í•œ ë¬¸ì¥"ì„ ì°¾ëŠ” ë°©ì‹ì„
#   ë³´í†µ "ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(semantic search)" ë¼ê³  ë¶€ë¦„.
# - ì—¬ê¸°ì„œëŠ” "ë¹„ìŠ·í•œ ì§ˆë¬¸"ì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©í•¨.

embedder_for_cache = OpenAIEmbeddings(model=EMBEDDING_MODEL)

# ë‚œì´ë„ë³„ë¡œ ìºì‹œë¥¼ ë”°ë¡œ ê´€ë¦¬
# - exact: ì§ˆë¬¸ ë¬¸ì¥ì´ ì™„ì „íˆ ë˜‘ê°™ì„ ë•Œ
# - semantic: ì§ˆë¬¸ ë¬¸ì¥ì€ ë‹¤ë¥´ì§€ë§Œ 'ëœ»'ì´ ë¹„ìŠ·í•  ë•Œ
CACHE = {
    "ì´ˆê¸‰": {"exact": {}, "semantic": []},
    "ì¤‘ê¸‰": {"exact": {}, "semantic": []},
    "ê³ ê¸‰": {"exact": {}, "semantic": []},
}


def cosine_similarity(a, b):
    """ë‘ ë²¡í„°(ìˆ«ì ë¦¬ìŠ¤íŠ¸)ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ 0~1 ì‚¬ì´ ì ìˆ˜ë¡œ ê³„ì‚°"""
    a, b = np.array(a), np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def is_quiz_like_question(q: str) -> bool:
    """
    ì´ ì§ˆë¬¸ì´ 'ë¬¸ì œ/í€´ì¦ˆë¥¼ ë‚´ë‹¬ë¼'ì— ê°€ê¹Œìš´ì§€ ê°„ë‹¨íˆ íŒë³„.
    - ì—¬ê¸°ì„œëŠ” ìºì‹œ(íŠ¹íˆ 'ë¹„ìŠ·í•œ ì§ˆë¬¸ ì°¾ê¸°')ë¥¼ ë§‰ê¸° ìœ„í•œ ìš©ë„ë¡œë§Œ ì‚¬ìš©.
    - ëª¨ë“œ(ì„¤ëª…/í€´ì¦ˆ) êµ¬ë¶„ì€ LLMì´ ë”°ë¡œ í•œë‹¤.
    """
    q2 = q.replace(" ", "")
    keywords = ["í€´ì¦ˆ", "ë¬¸ì œ", "í…ŒìŠ¤íŠ¸", "ì—°ìŠµë¬¸ì œ", "ox", "OX"]
    return any(k in q2 for k in keywords)


def search_cache(question: str, grade: str):
    """
    ìºì‹œì—ì„œ ë¨¼ì € ë‹µì„ ì°¾ì•„ë³´ëŠ” í•¨ìˆ˜.
    1) ì§ˆë¬¸ì´ ì™„ì „íˆ ê°™ìœ¼ë©´(exact) â†’ ë°”ë¡œ ë°˜í™˜
    2) í€´ì¦ˆ/ë¬¸ì œ ìš”ì²­ì´ë©´ â†’ 'ë¹„ìŠ·í•œ ì§ˆë¬¸' ì¬ì‚¬ìš©ì€ ìœ„í—˜í•´ì„œ ê±´ë„ˆëœ€
    3) ì„¤ëª… ìš”ì²­ì´ë©´ â†’ 'ëœ»ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸'ë„ ì°¾ì•„ë³´ê³  ì¶©ë¶„íˆ ë¹„ìŠ·í•˜ë©´ ì¬ì‚¬ìš©
    """
    # 1) ì™„ì „íˆ ê°™ì€ ì§ˆë¬¸ì¸ ê²½ìš° (ë¬¸ì¥ ê·¸ëŒ€ë¡œ ì¼ì¹˜)
    if question in CACHE[grade]["exact"]:
        print("[CACHE HIT] Exact")
        return CACHE[grade]["exact"][question]

    # 2) í€´ì¦ˆ/ë¬¸ì œ ìš”ì²­ì´ë©´, 'ë¹„ìŠ·í•œ ì§ˆë¬¸' ìºì‹œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    #    ì˜ˆ: "forë¬¸ ë¬¸ì œ 5ê°œ" vs "forë¬¸ ë¬¸ì œ 3ê°œ" â†’ ì„ì´ë©´ ì•ˆ ë¨
    if is_quiz_like_question(question):
        print("[CACHE SKIP] í€´ì¦ˆ ìš”ì²­ â†’ semantic ìºì‹œ ì‚¬ìš© ì•ˆ í•¨")
        return None

    # 3) ì„¤ëª… ìš”ì²­ì¸ ê²½ìš°ì—ë§Œ 'ëœ»ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸'ì„ ì°¾ì•„ë³¸ë‹¤.
    print("[CACHE CHECK] Semantic...")
    q_vec = embedder_for_cache.embed_query(question)

    best_score = 0
    best_answer = None

    for entry in CACHE[grade]["semantic"]:
        score = cosine_similarity(q_vec, entry["vector"])
        if score > best_score:
            best_score = score
            best_answer = entry["answer"]

    # 0.8 ì´ìƒì´ë©´ "ê½¤ ë¹„ìŠ·í•˜ë‹¤"ë¼ê³  ë³´ê³  ì¬ì‚¬ìš©
    if best_score >= 0.80:
        print(f"[CACHE HIT] Semantic score={best_score:.3f}")
        return best_answer

    return None


def save_to_cache(question: str, grade: str, answer: str):
    """ìƒˆë¡œ ë§Œë“  ë‹µë³€ì„ ìºì‹œì— ì €ì¥"""
    vec = embedder_for_cache.embed_query(question)

    # 1) ì™„ì „íˆ ê°™ì€ ì§ˆë¬¸ìš© ìºì‹œ
    CACHE[grade]["exact"][question] = answer

    # 2) 'ëœ»ì´ ë¹„ìŠ·í•œ ì§ˆë¬¸' ê²€ìƒ‰ìš© ìºì‹œ
    CACHE[grade]["semantic"].append({
        "question": question,
        "vector": vec,
        "answer": answer
    })

    print("[CACHE SAVE] ì™„ë£Œ")


# ==============================================================
# ì„¤ëª… ëª¨ë“œ í…œí”Œë¦¿
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì‰¬ìš´ ë§ë¡œ í•œ ì¤„ ì •ë¦¬.

[í•µì‹¬ ìš”ì•½]
- ì´ˆë³´ìë„ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´ ìµœì†Œí™”. ì¦‰ì‹œ í’€ì´ í¬í•¨.
- 2~4ì¤„ë¡œ ì„¤ëª….

[ë¹„ìœ  + ì˜ˆì‹œ ì½”ë“œ]
- í˜„ì‹¤ ë¹„ìœ  1~2ê°œ.
- ì½”ë“œ ì˜ˆì‹œëŠ” 3~7ì¤„, ë°˜ë“œì‹œ ```python ì½”ë“œë¸”ë¡```ìœ¼ë¡œ ì‘ì„±.

[ì¶”ê°€ ì„¤ëª…]
- 1~2ì¤„.

[ì—°ìŠµ ë¬¸ì œ]
- 1~2ê°œ, íŒíŠ¸ í¬í•¨. ë°˜ë“œì‹œ context ì•ˆì˜ ë‚´ìš© ê¸°ë°˜.

[ì¶œì²˜]
- ì‚¬ìš©í•œ ë‚´ìš©ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
""",

    "ì¤‘ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬.

[ì •í™•í•œ ì •ì˜]
- context ê¸°ë°˜ìœ¼ë¡œ 2~4ì¤„.

[ì™œ í•„ìš”í•œê°€]
- ì‹¤ë¬´/í•™ìŠµ ê´€ì ìœ¼ë¡œ 1~2ì¤„.

[ì£¼ì˜ í¬ì¸íŠ¸]
- í—·ê°ˆë¦¬ê¸° ì‰¬ìš´ ë¶€ë¶„ 1~3ê°œ bullet.

[ì˜ˆì‹œ ì½”ë“œ]
- 3~8ì¤„, ë°˜ë“œì‹œ ```python ì½”ë“œë¸”ë¡``` ì‚¬ìš©.

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
""",

    "ê³ ê¸‰": """
[í•µì‹¬ ìš”ì•½]
- ê°œë…ì˜ ë³¸ì§ˆì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.

[ë™ì‘ ì›ë¦¬]
- ë‚´ë¶€ êµ¬ì¡°/ë©”ì»¤ë‹ˆì¦˜ ì¤‘ì‹¬ ì„¤ëª….

[ì„±ëŠ¥/ë©”ëª¨ë¦¬]
- context ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª….

[ë¹„êµ]
- ìœ ì‚¬ ê¸°ìˆ ê³¼ì˜ ë¹„êµ 1~3ê°œ.

[ì‚¬ë¡€]
- 3~8ì¤„ì˜ ê³ ê¸‰ ì˜ˆì‹œ ì½”ë“œ(ì½”ë“œë¸”ë¡).

[ì¶œì²˜]
- íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸.
"""
}


# ==============================================================
# í•™ìŠµí€´ì¦ˆ ìƒì„± ëª¨ë“œ í…œí”Œë¦¿
# ==============================================================

QUIZ_RULES_TEMPLATE = """
[ëª¨ë“œ]
- ì´ ìš”ì²­ì€ 'í•™ìŠµí€´ì¦ˆ ìƒì„±'ì…ë‹ˆë‹¤.
- ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ì •ë‹µì€ context(ê°•ì˜ìë£Œ)ì— ê·¼ê±°í•´ì•¼ í•˜ì§€ë§Œ,
  í•™ìŠµ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•œ ë²”ìœ„ ë‚´ì—ì„œ ì•½ê°„ì˜ ì¼ë°˜ íŒŒì´ì¬ ì§€ì‹ í™•ì¥ì€ í—ˆìš©í•©ë‹ˆë‹¤.

[í—ˆìš© ë¬¸ì œ í˜•ì‹]
1) OX ë¬¸ì œ
2) ê°ê´€ì‹ ë¬¸ì œ(ë³´ê¸° 5ê°œ)

[ë¬¸ì œ ê°œìˆ˜ ê·œì¹™]
- ì‚¬ìš©ìê°€ ë¬¸ì œ ê°œìˆ˜ë¥¼ ì§ì ‘ ìš”ì²­í–ˆë‹¤ë©´, ìš”ì²­í•œ ê°œìˆ˜ë§Œ ìƒì„±í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë¬¸ì œ ê°œìˆ˜ë¥¼ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ 5ë¬¸ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

[ë¬¸ì œ ìœ í˜• ê¸°ë³¸ ê·œì¹™]
- ì‚¬ìš©ìê°€ ë¬¸ì œ ìœ í˜•ì„ íŠ¹ë³„íˆ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ê¸°ë³¸ì ìœ¼ë¡œ ê°ê´€ì‹(multiple) ë¬¸ì œ ìœ„ì£¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
- í•„ìš”í•œ ê²½ìš° OX ë¬¸ì œë¥¼ ì„ì–´ë„ ë˜ì§€ë§Œ, ë‚œì´ë„ì™€ í•™ìŠµ íš¨ê³¼ë¥¼ ê³ ë ¤í•´ì„œ ì ì ˆíˆ ì„ íƒí•©ë‹ˆë‹¤.

[ì½”ë“œê°€ ë“¤ì–´ê°€ëŠ” ë¬¸ì œ ê·œì¹™]
- ì½”ë“œê°€ í¬í•¨ëœ ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì½”ë“œë¸”ë¡( ```python ... ``` ) í˜•íƒœë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
- ì½”ë“œë¸”ë¡ ë‚´ë¶€ëŠ” ë“¤ì—¬ì“°ê¸°ì™€ ì¤„ë°”ê¿ˆì„ ì •í™•í•˜ê²Œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

[JSON ìŠ¤í‚¤ë§ˆ]
{
  "total": ë¬¸ì œìˆ˜,
  "items": [
    {
      "number": 1,
      "type": "ox" ë˜ëŠ” "multiple",
      "question": "ë¬¸ì œ ë‚´ìš© (í•„ìš”ì‹œ ì½”ë“œë¸”ë¡ í¬í•¨)",
      "choices": ["ë³´ê¸°1","ë³´ê¸°2","ë³´ê¸°3","ë³´ê¸°4","ë³´ê¸°5"] ë˜ëŠ” null,
      "answer": "ì •ë‹µ(ë˜ëŠ” ì •ë‹µ ë³´ê¸° ë‚´ìš©)",
      "difficulty": "{GRADE_LEVEL}",
      "source_file": "íŒŒì¼ëª…",
      "source_page": í˜ì´ì§€ë²ˆí˜¸
    }
  ]
}

[ë¬¸ì œ ìƒì„± ê·œì¹™]
- context ì•ˆì˜ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œë¥¼ ì¶œì œí•©ë‹ˆë‹¤.
- ë‹¨, ìˆ˜ê°•ìƒ í•™ìŠµì„ ë•ëŠ” ë²”ìœ„ ë‚´ì—ì„œëŠ” ì•½ê°„ì˜ í™•ì¥ë„ í—ˆìš©í•©ë‹ˆë‹¤.
- ê°ê´€ì‹ ë³´ê¸°ëŠ” ë°˜ë“œì‹œ 5ê°œì—¬ì•¼ í•©ë‹ˆë‹¤.
- ê°ê´€ì‹ ë¬¸ì œì˜ ì •ë‹µì€ ë°˜ë“œì‹œ ë³´ê¸° ì•ˆì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë¬¸ì œëŠ” ìˆ˜ê°•ìƒì´ í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.

[ì¶œë ¥ ê·œì¹™]
- JSONë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. (ì„¤ëª…, í•´ì„¤, ìì—°ì–´ ë¬¸ì¥ ì ˆëŒ€ ê¸ˆì§€)
"""


# ==============================================================
# ì§ˆë¬¸ ì˜ë„(ì„¤ëª…/í€´ì¦ˆ) LLMìœ¼ë¡œ íŒë‹¨
# ==============================================================

INTENT_PROMPT = """
ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë¶„ë¥˜ AIì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì•„ë˜ ë‘˜ ì¤‘ ë¬´ì—‡ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

1) "ì„¤ëª…" â†’ ê°œë… ì„¤ëª…, ì´í•´ë¥¼ ë•ëŠ” ë‹µë³€ì„ ì›í•˜ëŠ” ê²½ìš°
2) "í€´ì¦ˆ" â†’ í•™ìŠµì„ ìœ„í•œ ë¬¸ì œ(í€´ì¦ˆ)ë¥¼ ë§Œë“¤ì–´ë‹¬ë¼ëŠ” ê²½ìš°

ì¶œë ¥ í˜•ì‹:
- ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
  - ì„¤ëª…
  - í€´ì¦ˆ
"""


def detect_intent(question: str, llm: ChatOpenAI) -> str:
    """LLMì—ê²Œ 'ì„¤ëª…' ë˜ëŠ” 'í€´ì¦ˆ' ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ ë‹¬ë¼ê³  ìš”ì²­"""
    prompt = INTENT_PROMPT + f"\n\n[ì§ˆë¬¸]\n{question}\n"
    result = llm.invoke(prompt)   # AIMessage ê°ì²´ ë°˜í™˜
    text = result.content.strip()

    # í˜¹ì‹œë¼ë„ ì´ìƒí•œ ë‹µì´ ë‚˜ì˜¤ë©´ ê¸°ë³¸ê°’ì€ "ì„¤ëª…"
    if "í€´ì¦ˆ" in text:
        return "í€´ì¦ˆ"
    return "ì„¤ëª…"


def build_rules(question: str, grade: str, llm_for_intent: ChatOpenAI) -> str:
    """
    ì§ˆë¬¸ì„ ë³´ê³  'ì„¤ëª… ëª¨ë“œ'ë¡œ ê°ˆì§€, 'í€´ì¦ˆ ëª¨ë“œ'ë¡œ ê°ˆì§€ ì„ íƒí•œ ë’¤
    ê·¸ì— ë§ëŠ” ê·œì¹™ í…ìŠ¤íŠ¸ë¥¼ ëŒë ¤ì£¼ëŠ” í•¨ìˆ˜.
    """
    intent = detect_intent(question, llm_for_intent)  # "ì„¤ëª…" ë˜ëŠ” "í€´ì¦ˆ"

    if intent == "í€´ì¦ˆ":
        print("[MODE] í•™ìŠµí€´ì¦ˆ ëª¨ë“œ (LLM íŒë‹¨)")
        return QUIZ_RULES_TEMPLATE.replace("{GRADE_LEVEL}", grade)

    print("[MODE] ì„¤ëª… ëª¨ë“œ (LLM íŒë‹¨)")
    return GRADE_RULES[grade]


# ==============================================================
# metadata â†’ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
# ==============================================================

def format_docs_with_metadata(docs):
    """
    ë²¡í„°DBì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸(docs)ë¥¼
    - [1] ì¶œì²˜: íŒŒì¼ëª… / p.í˜ì´ì§€
    - ë³¸ë¬¸ ë‚´ìš©
    ì´ëŸ° í˜•íƒœì˜ í° ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜.
    """
    parts = []

    for idx, doc in enumerate(docs, start=1):
        meta = doc.metadata or {}

        file_name = (
            meta.get("filename")
            or meta.get("file_name")
            or meta.get("source")
            or meta.get("filename_eng")
            or "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"
        )

        page = meta.get("page") or meta.get("page_number") or meta.get("page_index")

        header = f"[{idx}] ì¶œì²˜: {file_name}"
        if page is not None:
            header += f" / p.{page}"

        body = doc.page_content or ""

        parts.append(f"{header}\n{body}")

    return "\n\n".join(parts)


# ==============================================================
# ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ê²€ìƒ‰ëŸ‰ ì¡°ì ˆ
# ==============================================================

def estimate_topic_count(question: str) -> int:
    """
    ì§ˆë¬¸ ì•ˆì— 'ê·¸ë¦¬ê³ , /, ì™€, ê³¼' ê°™ì€ ì—°ê²°ì–´ê°€ ëª‡ ê°œ ìˆëŠ”ì§€ ëŒ€ëµ ì„¸ì–´ì„œ
    - í•œ ê°€ì§€ ì£¼ì œì¸ì§€
    - ë‘ì„¸ ê°€ì§€ë¥¼ í•œ ë²ˆì— ë¬¼ì–´ë³´ëŠ”ì§€
    ë¥¼ ì•„ì£¼ ë‹¨ìˆœí•˜ê²Œ ì¶”ì •.
    """
    joiners = ["ì™€ ", "ê³¼ ", "ì´ë‘", "ë‘", " ë° ", " ê·¸ë¦¬ê³  ", ",", "/"]
    score = 1
    for j in joiners:
        if j in question:
            score += 1
    return max(1, min(score, 3))  # 1~3 ì‚¬ì´ë¡œ ì œí•œ


def adjust_retriever_for_question(question: str):
    """
    ì§ˆë¬¸ì´ ì—¬ëŸ¬ ì£¼ì œë¥¼ ì„ì–´ì„œ ë¬¼ì–´ë³´ëŠ” ê²ƒ ê°™ìœ¼ë©´
    í•œ ë²ˆì— ê°€ì ¸ì˜¤ëŠ” ë¬¸ì„œ ìˆ˜(k, fetch_k)ë¥¼ ëŠ˜ë ¤ì£¼ëŠ” í•¨ìˆ˜.
    """
    global RETRIEVER
    if RETRIEVER is None:
        return

    t = estimate_topic_count(question)

    if t == 1:
        k, f = 3, 8
    elif t == 2:
        k, f = 6, 16
    else:
        k, f = 9, 24

    RETRIEVER.search_kwargs["k"] = k
    RETRIEVER.search_kwargs["fetch_k"] = f

    print(f"[RETRIEVER] topic={t} â†’ k={k}, fetch_k={f}")


# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™”
# ==============================================================

def initialize_rag_chain():
    """
    - ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    - Chroma ë²¡í„°DB ì—°ê²°
    - Retriever ìƒì„±
    - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + LLM ì—°ê²°
    - ì „ì²´ íŒŒì´í”„ë¼ì¸(chain) êµ¬ì„±
    """
    global RETRIEVER

    print("[LOG] RAG ì²´ì¸ ì´ˆê¸°í™”â€¦")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION
    )

    RETRIEVER = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ì…ë‹ˆë‹¤.

[ì—­í• ]
- ì„¤ëª… ëª¨ë“œì¼ ë•Œ: ë°˜ë“œì‹œ [Context] ì•ˆì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
- í€´ì¦ˆ ëª¨ë“œì¼ ë•Œ: ë¬¸ì œëŠ” contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ê³ ,
  í•„ìš”í•˜ë‹¤ë©´ í•™ìŠµ íš¨ê³¼ë¥¼ ìœ„í•´ ê°€ë³ê²Œ í™•ì¥í•  ìˆ˜ ìˆì§€ë§Œ,
  ì •ë‹µì€ context ì•ˆì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ê·œì¹™]
{rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    chain = (
        {
            "docs": itemgetter("question") | RETRIEVER,
            "question": itemgetter("question"),
            "rules": itemgetter("rules"),
            "grade": itemgetter("grade"),
            "history": itemgetter("history"),
        }
        # docsë¥¼ ë°›ì•„ì„œ context ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” ë‹¨ê³„
        | RunnableLambda(lambda x: {
            **x,
            "context": format_docs_with_metadata(x["docs"])
        })
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] init ì™„ë£Œ: {time.time() - start:.2f}s")
    return chain


def get_rag_chain():
    """RAG ì²´ì¸ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ì¬ì‚¬ìš©"""
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================
# HISTORY (ë©€í‹°í„´)
# ==============================================================

def build_history_text(history, max_turns=2):
    """
    ì´ì „ ëŒ€í™” ë‚´ìš© ë¦¬ìŠ¤íŠ¸(history)ë¥¼
    "í•™ìƒ: ... / AI: ..." í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜.
    ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìµœê·¼ max_turnsë²ˆë§Œ ì‚¬ìš©.
    """
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join(
        [f"í•™ìƒ: {h['question']}\nAI: {h['answer']}" for h in recent]
    )


# ==============================================================
# ë©”ì¸ ë‹µë³€ í•¨ìˆ˜
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    """
    ì‹¤ì œë¡œ 1ê°œì˜ ì§ˆë¬¸ì— ëŒ€í•´ 1ê°œì˜ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜.
    - ìºì‹œ í™•ì¸
    - ê²€ìƒ‰ëŸ‰ ì¡°ì •
    - ì„¤ëª…/í€´ì¦ˆ ëª¨ë“œ íŒë‹¨
    - RAG ì²´ì¸ í˜¸ì¶œ
    - ìºì‹œì— ì €ì¥ + history ì—…ë°ì´íŠ¸
    """

    # 1) ìºì‹œ í™•ì¸
    cached = search_cache(question, grade)
    if cached:
        print("[INFO] ìºì‹œ ì‚¬ìš©")
        history.append({"question": question, "answer": cached})
        return cached

    # 2) ê²€ìƒ‰ëŸ‰ ì¡°ì •
    adjust_retriever_for_question(question)

    # 3) LLMì—ê²Œ 'ì„¤ëª…/í€´ì¦ˆ' ì˜ë„ íŒë‹¨ ë§¡ê¸°ê¸°
    llm_for_intent = ChatOpenAI(model=LLM_MODEL, temperature=0.0)
    rules_text = build_rules(question, grade, llm_for_intent)

    # 4) RAG ì²´ì¸ ì¤€ë¹„
    rag = get_rag_chain()
    history_text = build_history_text(history)

    # 5) ì‹¤ì œ ë‹µë³€ ìƒì„±
    start = time.time()
    answer = rag.invoke({
        "question": question,
        "grade": grade,
        "rules": rules_text,
        "history": history_text
    })
    print(f"[Time] ë‹µë³€ ìƒì„±: {time.time() - start:.3f}s")

    # 6) ìºì‹œì— ì €ì¥ + history ì—…ë°ì´íŠ¸
    save_to_cache(question, grade, answer)
    history.append({"question": question, "answer": answer})

    return answer


# ==============================================================
# CLI ì‹¤í–‰ë¶€ (í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ìš©)
# ==============================================================

if __name__ == "__main__":
    print("\n=== RAG í•™ìŠµ ë„ìš°ë¯¸ ì±—ë´‡ (ì„¤ëª… + í•™ìŠµí€´ì¦ˆ ìë™ ë¶„ê¸°) ===\n")
    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ").strip()
        if msg.lower() == "exit":
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        grade = input("ğŸ’¡ ë‚œì´ë„ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        print("\nâ³ ìƒì„± ì¤‘...\n")
        result = answer_single(msg, grade, history)

        print("\nğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ì‘ë‹µ:\n")
        print(result)
        print("\n-----------------------------------\n")
