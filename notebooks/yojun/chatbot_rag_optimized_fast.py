# chatbot_rag_optimized.py
# metadata íŒ¨í‚¹ + êµ¬ì¡°í™” í…œí”Œë¦¿ + ìµœì í™”ëœ RAG ë²„ì „

import os
import time
from dotenv import load_dotenv
from operator import itemgetter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"           # ì†ë„+ì •í™•ë„ ê· í˜•
EMBEDDING_MODEL = "text-embedding-3-large"  # ê¸°ì¡´ DBì™€ ì°¨ì› ë§ì¶¤

SEARCH_K = 3       # ìµœì í™”ëœ ê²€ìƒ‰ ê°œìˆ˜
FETCH_K = 8        # í›„ë³´ ê°œìˆ˜

RAG_CHAIN = None   # ì „ì—­ ìºì‹±ìš©


# ==============================================================
# êµ¬ì¡°í™” í…œí”Œë¦¿ (ì´ˆê¸‰ / ì¤‘ê¸‰ / ê³ ê¸‰)
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ì‰¬ìš´ ë§ë¡œ í•œ ì¤„ë¡œ ë‹¤ì‹œ ì •ë¦¬í•©ë‹ˆë‹¤.

[í•µì‹¬ í•œ ì¤„ ìš”ì•½]
- ì´ ê°œë…ì„ ì´ˆë³´ìê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

[ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´, ì˜ì–´, ì¶•ì•½ì–´ëŠ” ìµœëŒ€í•œ í”¼í•˜ê³ 
  í•„ìš”í•  ë•ŒëŠ” ê´„í˜¸ë¡œ ì¦‰ì‹œ í’€ì´í•©ë‹ˆë‹¤.
  ì˜ˆ: â€œë¼ì´ë¸ŒëŸ¬ë¦¬(ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ê¸°ëŠ¥ ë¬¶ìŒ)â€
- ì„¤ëª…ì€ 2~4ì¤„ë¡œ ê°„ë‹¨í•˜ê³  ì§ê´€ì ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

[ë¹„ìœ  + ì˜ˆì‹œ ì½”ë“œ]
- í˜„ì‹¤ ì„¸ê³„ì˜ ë¹„ìœ ë¥¼ 1~2ê°œ ì œê³µí•©ë‹ˆë‹¤.
  ì¡°ê±´:
    1) ìƒí™œ ì† ë¬¼ê±´, ìŒì‹, ê¸°ì¡´ ê²½í—˜ ë“± ì´ˆë³´ìê°€ ë°”ë¡œ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ëŒ€ìƒ
    2) ë¹„ìœ ê°€ ê°œë…ê³¼ 1:1ë¡œ ì—°ê²°ë˜ë„ë¡ ì„¤ëª…

- ë¹„ìœ  â†’ ì½”ë“œ ê°œë… ì—°ê²° ê³¼ì •ì„ ëª…í™•íˆ ì ìŠµë‹ˆë‹¤.
  ì˜ˆ: â€œìš”ë¦¬ ë ˆì‹œí”¼ì— ì¬ë£Œë¥¼ ë„£ìœ¼ë©´ ê²°ê³¼ê°€ ë‚˜ì˜¤ë“¯,
       í•¨ìˆ˜ì— ì…ë ¥ì„ ë„£ìœ¼ë©´ ì¶œë ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.â€

- ì´ˆë³´ìê°€ ë”°ë¼ ì¹  ìˆ˜ ìˆëŠ” ì§§ì€ ì½”ë“œ ì˜ˆì‹œ(3~7ì¤„)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

[ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ì ]
- ë¶€ë‹´ë˜ì§€ ì•Šì„ ì •ë„ì˜ ì¶”ê°€ ì„¤ëª… 1~2ì¤„

[ì—°ìŠµ ë¬¸ì œ]
- ì´ˆë³´ìê°€ í’€ ìˆ˜ ìˆëŠ” ì§§ì€ ì—°ìŠµ ë¬¸ì œ 1~2ê°œ ì œì‹œ
- ê° ë¬¸ì œì— 1ì¤„ íŒíŠ¸ ì œê³µ

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
  ì˜ˆ: â€œ01 íŒŒì´ì¬ ê¸°ì´ˆ ë¬¸ë²• I.pdf / p.3â€
""",

    "ì¤‘ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.

[ì •í™•í•œ ì •ì˜]
- context ê¸°ë°˜ìœ¼ë¡œ 2~4ì¤„ ì•ˆì—ì„œ ì •ì˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•„ìš” ì‹œ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥ (ë¶ˆí•„ìš”í•˜ê²Œ í™•ì¥ ê¸ˆì§€)

[ì™œ í•„ìš”í•œê°€]
- ì´ ê°œë…ì´ ì™œ ì¤‘ìš”í•œì§€ ì‹¤ë¬´ ë˜ëŠ” í•™ìŠµ ê´€ì ì—ì„œ 1~2ì¤„ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

[ì‹¤ë¬´ ì£¼ì˜ í¬ì¸íŠ¸]
- ì‹¤ë¬´ ë˜ëŠ” í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ì‹¤ìˆ˜í•˜ëŠ” ë¶€ë¶„, í—·ê°ˆë¦¬ëŠ” í¬ì¸íŠ¸ 1~3ê°œ ì œì‹œ

[ì˜ˆì‹œ ì½”ë“œ ë˜ëŠ” ê°„ë‹¨ ì˜ˆì œ]
- ì¤‘ê¸‰ì ìˆ˜ì¤€ì˜ ì½”ë“œ ì˜ˆì‹œë¥¼ 3~8ì¤„ ì œê³µ

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
""",

    "ê³ ê¸‰": """
[í•µì‹¬ ê°œë… ìš”ì•½]
- ê°œë…ì˜ ë³¸ì§ˆì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.

[ë‚´ë¶€ ë™ì‘ ì›ë¦¬]
- êµ¬ì¡°, ë©”ì»¤ë‹ˆì¦˜, íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ì›ë¦¬ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.
- context ê¸°ë°˜ìœ¼ë¡œ ì„œìˆ í•˜ë©° ë¶ˆí•„ìš”í•œ ì™¸ë¶€ ì§€ì‹ í™•ì¥ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.

[ì„±ëŠ¥/ë©”ëª¨ë¦¬/íš¨ìœ¨ì„± ê´€ì ]
- ê°€ëŠ¥í•œ ê²½ìš° ì‹œê°„ ë³µì¡ë„, ë©”ëª¨ë¦¬ ì‚¬ìš©, ì²˜ë¦¬ êµ¬ì¡° ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
- contextì— ì¡´ì¬í•˜ëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

[ë¹„êµ]
- ìœ ì‚¬ ê°œë… ë˜ëŠ” ëŒ€ì•ˆ ê¸°ìˆ ê³¼ì˜ ì°¨ì´ë¥¼ 1~3ê°œ bulletë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

[ì˜ˆì‹œ ë˜ëŠ” ì ìš© ì‚¬ë¡€]
- ê³ ê¸‰ìì—ê²Œ ì í•©í•œ ì˜ˆì‹œ ë˜ëŠ” ê¸°ìˆ  ì ìš© ì‚¬ë¡€ë¥¼ 3~8ì¤„ ì‚¬ì´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

[ì¶œì²˜]
- ì‚¬ìš©ëœ context chunkë“¤ì˜ íŒŒì¼ëª… + í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™œíˆ í‘œê¸°í•©ë‹ˆë‹¤.
"""
}


# ==============================================================
# metadata â†’ í…ìŠ¤íŠ¸ë¡œ íŒ¨í‚¹í•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜
# ==============================================================

def format_docs_with_metadata(docs):
    """
    retrieverê°€ ë°˜í™˜í•œ Document ë¦¬ìŠ¤íŠ¸ë¥¼
    [ì¶œì²˜ ì •ë³´ + ë‚´ìš©] í˜•íƒœì˜ ê¸´ ë¬¸ìì—´ë¡œ ë³€í™˜í•œë‹¤.

    ê° ë¬¸ì„œëŠ” ëŒ€ëµ ì´ëŸ° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨:

    [1] ì¶œì²˜: 03 ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ - íŒë‹¤ìŠ¤.pdf / p.5
    ë¬¸ì„œ ë‚´ìš©...

    [2] ì¶œì²˜: 01 íŒŒì´ì¬ ê¸°ì´ˆ ë¬¸ë²• I.pdf / p.3
    ë¬¸ì„œ ë‚´ìš©...
    """
    parts = []

    for idx, doc in enumerate(docs, start=1):
        meta = doc.metadata or {}

        # íŒŒì¼ëª… í›„ë³´ í‚¤ë“¤
        file_name = (
            meta.get("file_name")
            or meta.get("source")
            or meta.get("filename")
            or "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"
        )

        # í˜ì´ì§€ ë²ˆí˜¸ í›„ë³´ í‚¤ë“¤
        page = (
            meta.get("page")
            or meta.get("page_number")
            or meta.get("page_index")
        )

        if page is not None:
            header = f"[{idx}] ì¶œì²˜: {file_name} / p.{page}"
        else:
            header = f"[{idx}] ì¶œì²˜: {file_name}"

        body = doc.page_content or ""
        parts.append(f"{header}\n{body}")

    return "\n\n".join(parts)


# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™” + ìºì‹±
# ==============================================================

def initialize_rag_chain():
    print("[LOG] RAG ì²´ì¸ ì´ˆê¸°í™” ì‹œì‘")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION,
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ [Context] ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜(íŒŒì¼ëª…, í˜ì´ì§€)ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ë‹µë³€ ê·œì¹™]
{grade_rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
ìœ„ êµ¬ì¡°ì™€ ê·œì¹™ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.
"""

    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    # 1) question â†’ retriever â†’ docs
    # 2) docsë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´(context)ë¡œ ë³€í™˜
    # 3) promptì— ì „ë‹¬
    chain = (
        {
            "docs": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "grade": itemgetter("grade"),
            "grade_rules": itemgetter("grade_rules"),
            "history": itemgetter("history"),
        }
        | RunnableLambda(
            lambda x: {
                **x,
                "context": format_docs_with_metadata(x["docs"])
            }
        )
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] RAG ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ: {time.time() - start:.3f}ì´ˆ")
    return chain


def get_rag_chain():
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================
# HISTORY (ë©€í‹°í„´)
# ==============================================================

def build_history_text(history, max_turns=2):
    """
    ìµœê·¼ max_turns ê°œì˜ ì§ˆë¬¸/ë‹µë³€ë§Œ ì‚¬ìš©í•´ LLMì— ì „ë‹¬.
    """
    if not history:
        return ""
    recent = history[-max_turns:]
    return "\n".join(
        [f"í•™ìƒ: {h['question']}\nAI: {h['answer']}\n" for h in recent]
    )


# ==============================================================
# ì§ˆë¬¸ ë¶„ë¦¬ (ì—¬ëŸ¬ ìš”ì²­ì´ ì„ì—¬ ìˆì„ ë•Œ)
# ==============================================================

def split_questions(user_message: str):
    """
    "ë¦¬ìŠ¤íŠ¸ ì„¤ëª…í•´ì£¼ê³ , í•¨ìˆ˜ ì˜ˆì œë„ ë³´ì—¬ì¤˜" ê°™ì€ ì…ë ¥ì„
    1. ë¦¬ìŠ¤íŠ¸ ì„¤ëª…í•´ì¤˜
    2. í•¨ìˆ˜ ì˜ˆì œë„ ë³´ì—¬ì¤˜
    ì´ëŸ° ì‹ìœ¼ë¡œ ë¶„ë¦¬.
    """
    splitter = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = ChatPromptTemplate.from_template(
        """
ì‚¬ìš©ìì˜ ì…ë ¥ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.

1. ì§ˆë¬¸1
2. ì§ˆë¬¸2

ë„ˆë¬´ ì˜ê²Œ ìª¼ê°œì§€ ë§ê³ , ì˜ë¯¸ ë‹¨ìœ„ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ëˆ„ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥:
{message}
"""
    )

    chain = prompt | splitter | StrOutputParser()
    raw = chain.invoke({"message": user_message})

    questions = []

    for line in raw.splitlines():
        line = line.strip()
        if line and line[0].isdigit() and "." in line:
            try:
                _, q = line.split(".", 1)
                q = q.strip()
                if q:
                    questions.append(q)
            except ValueError:
                continue

    if not questions:
        return [user_message.strip()]

    return questions


# ==============================================================
# ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    """
    í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´:
    - history ë°˜ì˜
    - RAG ê²€ìƒ‰ + LLM ë‹µë³€
    - ë‹µë³€ì„ historyì— ì €ì¥
    """
    if grade not in GRADE_RULES:
        raise ValueError("gradeëŠ” 'ì´ˆê¸‰', 'ì¤‘ê¸‰', 'ê³ ê¸‰' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    rag = get_rag_chain()
    history_text = build_history_text(history)

    start = time.time()
    answer = rag.invoke({
        "question": question,
        "grade": grade,
        "grade_rules": GRADE_RULES[grade],
        "history": history_text,
    })
    print(f"[Time] LLM ë‹µë³€ ìƒì„±: {time.time() - start:.3f}ì´ˆ")

    history.append({"question": question, "answer": answer})
    return answer


# ==============================================================
# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
# ==============================================================

def multi_answer(user_message: str, grade: str, history: list):
    """
    ì—¬ëŸ¬ ì§ˆë¬¸ì´ ì„ì¸ ê²½ìš°:
    1) split_questionsë¡œ ë‚˜ëˆ„ê³ 
    2) ê° ì§ˆë¬¸ë§ˆë‹¤ answer_single í˜¸ì¶œ
    3) ë³´ê¸° ì¢‹ê²Œ ë¬¶ì–´ì„œ ë°˜í™˜
    """
    questions = split_questions(user_message)

    # ì§ˆë¬¸ì´ í•˜ë‚˜ë©´ ë‹¨ì¼ ì²˜ë¦¬
    if len(questions) == 1:
        return answer_single(questions[0], grade, history)

    blocks = []
    for idx, q in enumerate(questions, start=1):
        ans = answer_single(q, grade, history)
        block = f"""### ì§ˆë¬¸ {idx}
> {q}

{ans}

---
"""
        blocks.append(block)

    return "\n".join(blocks)


# ==============================================================
# CLI í…ŒìŠ¤íŠ¸ìš© main
# ==============================================================

if __name__ == "__main__":
    print("\n=== metadata íŒ¨í‚¹ + êµ¬ì¡°í™” í…œí”Œë¦¿ RAG ì±—ë´‡ ===\n")
    print("ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì¨ë„ ë˜ê³ , í•˜ë‚˜ì”© ë¬¼ì–´ë´ë„ ë©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ").strip()
        if msg.lower() == "exit":
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        grade = input("ğŸ’¡ ë‚œì´ë„ ì„ íƒ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()
        if grade.lower() == "exit":
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘...\n")
        result = multi_answer(msg, grade, history)

        print("\nğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ë‹µë³€:\n")
        print(result)
        print("\n============================================")