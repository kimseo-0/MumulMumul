# chatbot_rag_optimized.py

import time
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

SEARCH_K = 5
FETCH_K = 20

# RAG ì²´ì¸ì„ í•œ ë²ˆë§Œ ë§Œë“¤ê³  ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜
RAG_CHAIN = None


# ==============================================================
# ìˆ˜ì¤€ë³„ ë‹µë³€ ê·œì¹™
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì´ˆê¸‰ì ë‹µë³€ ê·œì¹™]

ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë°/ë°ì´í„° ë¶„ì•¼ë¥¼ ì²˜ìŒ ë°°ìš°ëŠ” ì´ˆê¸‰ìë¥¼ ë•ëŠ” í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì„¤ëª…ì€ ë°˜ë“œì‹œ ì‰¬ìš´ í•œêµ­ì–´ë¡œ, ì§§ì€ ë¬¸ì¥ ìœ„ì£¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ì•„ë˜ 6ë‹¨ê³„ í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.

-------------------------------------

1) [ì§ˆë¬¸ ì´í•´]
- ì‚¬ìš©ìê°€ ì•Œê³  ì‹¶ì–´í•˜ëŠ” ë‚´ìš©ì„ í•œ ì¤„ë¡œ ë‹¤ì‹œ ì •ë¦¬í•©ë‹ˆë‹¤.
- ì „ë¬¸ìš©ì–´ ì—†ì´, ì‰¬ìš´ í•œêµ­ì–´ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

2) [í•µì‹¬ í•œ ì¤„ ìš”ì•½]
- ê²°ë¡ ì„ ê°€ì¥ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ í•œ ë¬¸ì¥ì— ìš”ì•½í•©ë‹ˆë‹¤.
- ì´ˆê¸‰ìê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

3) [ì‰¬ìš´ ì„¤ëª…]
- ì–´ë ¤ìš´ ìš©ì–´, ì˜ì–´, ì¶•ì•½ì–´ëŠ” ìµœëŒ€í•œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë¶€ë“ì´í•˜ê²Œ ì „ë¬¸ìš©ì–´ê°€ ë“±ì¥í•˜ë©´:
  â†’ ì¦‰ì‹œ ê´„í˜¸ ì•ˆì— ì‰¬ìš´ ëœ»ì„ ì ìŠµë‹ˆë‹¤.
  ì˜ˆ: â€œë¼ì´ë¸ŒëŸ¬ë¦¬(ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ê¸°ëŠ¥ ë¬¶ìŒ)â€

4) [ë¹„ìœ  / ì˜ˆì‹œ]
- í˜„ì‹¤ ë¹„ìœ  1ê°œ ì´ìƒì„ ì œê³µí•©ë‹ˆë‹¤.
- ì˜ˆì‹œ ì½”ë“œ 1ê°œë¥¼ ì œê³µí•˜ë˜, ë„ˆë¬´ ê¸¸ê²Œ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.

5) [ì¶”ê°€ë¡œ ì•Œë©´ ì¢‹ì€ ê²ƒ]
- ì´ˆê¸‰ìê°€ ë¶€ë‹´ ì—†ì´ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆì„ ì •ë„ë¡œ 1~2ì¤„ë§Œ í™•ì¥ ì„¤ëª…í•©ë‹ˆë‹¤.

6) [ì¶œì²˜]
- ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì •í™•í•˜ê²Œ í‘œê¸°í•©ë‹ˆë‹¤.
  íŒŒì¼ëª….pdf / p.ìˆ«ì ë˜ëŠ” p.ìˆ«ìâ€“ìˆ«ì

-------------------------------------

[íŠ¹ë³„ ì£¼ì˜ ì‚¬í•­]
- ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤.
- ì´ˆê¸‰ìê°€ ëª¨ë¥¼ ë§Œí•œ ê°œë…ì€ ë°˜ë“œì‹œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
- context(ê°•ì˜ìë£Œ)ì— ì—†ëŠ” ë‚´ìš©ì€ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
""",
    "ì¤‘ê¸‰": """
- ê°œë…ì˜ í•µì‹¬ ì •ì˜ë¥¼ ì •í™•í•˜ê²Œ ì œê³µ
- í•„ìš” ì‹œ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ ë¶ˆí•„ìš”í•œ í™•ì¥ ê¸ˆì§€
- ì™œ ì´ëŸ° ê°œë…ì´ í•„ìš”í•œì§€ 1ë²ˆ ì„¤ëª…
- ì‹¤ë¬´ì—ì„œ í—·ê°ˆë¦¬ëŠ” í¬ì¸íŠ¸ë„ í•¨ê»˜ ì œê³µ
- íŒŒì¼ëª…, í˜ì´ì§€ ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ í•¨ê»˜ ëª…ì‹œ
""",
    "ê³ ê¸‰": """
- ë‚´ë¶€ ë™ì‘ ì›ë¦¬ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…
- êµ¬ì¡°, ë©”ì»¤ë‹ˆì¦˜, ë©”ëª¨ë¦¬Â·ì„±ëŠ¥ ë“± ì‹¬í™” ë‚´ìš© í¬í•¨ ê°€ëŠ¥
- í•„ìš”í•œ ê²½ìš° ìˆ˜ì‹Â·ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥
- ë‹¤ë¥¸ ê¸°ìˆ ê³¼ ë¹„êµ ì„¤ëª… ê°€ëŠ¥
- íŒŒì¼ëª…, í˜ì´ì§€ ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ í•¨ê»˜ ëª…ì‹œ
"""
}


# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™” & ì¬ì‚¬ìš©
# ==============================================================

def initialize_rag_chain():
    """
    Chroma ë²¡í„°DB + OpenAI ì„ë² ë”© + RAG ì²´ì¸ ì´ˆê¸°í™”
    """
    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)

    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION,
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (history + context + grade ë°˜ì˜)
    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ [Context] ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ë‹µë³€ ê·œì¹™]
{grade_rules}

[ë‹µë³€ ì¡°ê±´]
- ì„¤ëª…ì€ ë°˜ë“œì‹œ í•™ìƒ ìˆ˜ì¤€ì— ë§ì¶°ì„œ ì‘ì„±
- ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì¶œì²˜(íŒŒì¼ëª…, í˜ì´ì§€ ë“±) ë°˜ë“œì‹œ ëª…ì‹œ
- Context ë°”ê¹¥ ì •ë³´ëŠ” ì‚¬ìš© ê¸ˆì§€

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""
    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    rag_chain = (
        {
            # ì§ˆë¬¸ ë¬¸ìì—´ë§Œ retrieverì— ì „ë‹¬
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "grade": itemgetter("grade"),
            "grade_rules": itemgetter("grade_rules"),
            "history": itemgetter("history"),
        }
        | prompt
        | model
        | StrOutputParser()
    )
    return rag_chain


def get_rag_chain():
    """
    RAG ì²´ì¸ì„ ì „ì—­ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©.
    """
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN


# ==============================================================
# History ìœ í‹¸ í•¨ìˆ˜ (ë©€í‹°í„´ìš©)
# ==============================================================

def build_history_text(history, max_turns=3):
    """
    ìµœê·¼ max_turnsê°œì˜ (ì§ˆë¬¸, ë‹µë³€)ì„ history ë¬¸ìì—´ë¡œ ë§Œë“ ë‹¤.
    LLMì´ ì´ì „ ëŒ€í™” íë¦„ì„ ì´í•´í•˜ëŠ” ë° ì‚¬ìš©.
    """
    if not history:
        return ""

    recent = history[-max_turns:]
    lines = []
    for turn in recent:
        lines.append(f"í•™ìƒ: {turn['question']}")
        lines.append(f"AI: {turn['answer']}")
        lines.append("")

    return "\n".join(lines)


# ==============================================================
# ì§ˆë¬¸ ë¶„ë¦¬ (ì—¬ëŸ¬ ìš”ì²­ì´ ì„ì—¬ ìˆì„ ë•Œ)
# ==============================================================

def split_questions(user_message: str):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì—ì„œ 'ì„œë¡œ ë‹¤ë¥¸ ì§ˆë¬¸/ìš”ì²­'ì„ ì˜ë¯¸ ë‹¨ìœ„ë³„ë¡œ ë¶„ë¦¬í•œë‹¤.
    ì˜ˆ:
      "ë¦¬ìŠ¤íŠ¸ ë¬¸ì œ 1ê°œ ë‚´ì£¼ê³  RAG ì½”ë“œë„ ë³´ì—¬ì¤˜"
    -> ["ë¦¬ìŠ¤íŠ¸ ë¬¸ì œ 1ê°œ ë‚´ì¤˜", "RAG ì½”ë“œë„ ë³´ì—¬ì¤˜"]
    """
    splitter = ChatOpenAI(model=LLM_MODEL, temperature=0)

    split_prompt = ChatPromptTemplate.from_template(
        """
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë³´ê³ , ì„œë¡œ ë‹¤ë¥¸ ìš”ì²­ì´ë‚˜ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ í•­ëª©ë³„ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:

1. ì²« ë²ˆì§¸ ì§ˆë¬¸...
2. ë‘ ë²ˆì§¸ ì§ˆë¬¸...
3. ì„¸ ë²ˆì§¸ ì§ˆë¬¸...

ê°€ëŠ¥í•˜ë©´ ìµœëŒ€í•œ ì˜ê²Œ ë‚˜ëˆ„ì§€ ë§ê³ ,
ì˜ë¯¸ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ëˆ ì§€ë„ë¡ ë¶„ë¦¬í•˜ì„¸ìš”.

ì‚¬ìš©ì ì…ë ¥:
{message}
"""
    )

    chain = split_prompt | splitter | StrOutputParser()
    raw = chain.invoke({"message": user_message})

    questions = []
    for line in raw.splitlines():
        line = line.strip()
        if not line:
            continue
        # "1. ì§ˆë¬¸..." í˜•ì‹ë§Œ íŒŒì‹±
        if line[0].isdigit() and "." in line:
            _, q = line.split(".", 1)
            q = q.strip()
            if q:
                questions.append(q)

    if not questions:
        return [user_message.strip()]

    return questions


# ==============================================================
# ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (history ë°˜ì˜)
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    """
    í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´:
    - history(ì´ì „ ëŒ€í™”)ë¥¼ ë°˜ì˜
    - RAG ê²€ìƒ‰ + LLM ë‹µë³€
    - ë‹µë³€ì„ historyì— ì €ì¥
    """
    if grade not in GRADE_RULES:
        raise ValueError("gradeëŠ” 'ì´ˆê¸‰', 'ì¤‘ê¸‰', 'ê³ ê¸‰' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    rag = get_rag_chain()
    history_text = build_history_text(history)

    answer_text = rag.invoke({
        "question": question,
        "grade": grade,
        "grade_rules": GRADE_RULES[grade],
        "history": history_text,
    })

    # history ì €ì¥
    history.append({
        "question": question,
        "answer": answer_text
    })

    return answer_text


# ==============================================================
# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬ (ì§ˆë¬¸ë³„ë¡œ ê°ê° RAG + ì¶œì²˜)
# ==============================================================

def multi_answer(user_message: str, grade: str, history: list):
    """
    í•œ ë²ˆì— ì—¬ëŸ¬ ì§ˆë¬¸ì´ ì„ì—¬ ìˆì„ ìˆ˜ ìˆëŠ” user_messageë¥¼ ë°›ì•„ì„œ:
    1) ì§ˆë¬¸ë“¤ì„ ë¶„ë¦¬í•˜ê³ 
    2) ê° ì§ˆë¬¸ë§ˆë‹¤ answer_single()ë¡œ ë‹µë³€ ìƒì„±
    3) ë³´ê¸° ì¢‹ê²Œ ë¬¶ì–´ì„œ ë°˜í™˜
    """
    questions = split_questions(user_message)

    # ì§ˆë¬¸ì´ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬
    if len(questions) == 1:
        return answer_single(questions[0], grade, history)

    blocks = []
    for idx, q in enumerate(questions, start=1):
        ans = answer_single(q, grade, history)
        block = f"""### ì§ˆë¬¸ {idx}
> {q}

{ans}

-------------------------------------
"""
        blocks.append(block)

    return "\n".join(blocks)


# ==============================================================
# í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ìš© main ë£¨í”„
# ==============================================================

if __name__ == "__main__":
    print("\n=== ë¶€íŠ¸ìº í”„ í•™ìŠµ ë„ìš°ë¯¸ RAG ì±—ë´‡ ===")
    print("ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì¨ë„ ë˜ê³ , í•œ ê°œì”© ë¬¼ì–´ë´ë„ ë©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    history = []  # ë©€í‹°í„´ ëŒ€í™” ê¸°ë¡ (ë‚˜ì¤‘ì— DBë¡œ í™•ì¥ ê°€ëŠ¥)

    while True:
        user_msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥: ")
        if user_msg.strip().lower() == "exit":
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        grade = input("ğŸ’¡ ë‚œì´ë„ ì„ íƒ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()
        if grade.strip().lower() == "exit":
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘...\n")

        # ë©€í‹° ì§ˆë¬¸ + ë©€í‹°í„´ + RAG + ì¶œì²˜ê¹Œì§€ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… í˜¸ì¶œ
        result = multi_answer(user_message=user_msg, grade=grade, history=history)

        print("ğŸ§  í•™ìŠµ ë„ìš°ë¯¸ ë‹µë³€:\n")
        print(result)
        print("\n============================================")
