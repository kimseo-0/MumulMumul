# chatbot_rag_optimized.py (ì‹œê°„ ì¸¡ì • í¬í•¨ ë²„ì „)

import os
import time
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

load_dotenv()

# ==============================================================
# ê¸°ë³¸ ì„¤ì •
# ==============================================================

DB_PATH = r"C:\POTENUP\MumulMumul\storage\vectorstore\curriculum_all_new"
COLLECTION = "curriculum_all_new"

LLM_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-large"

SEARCH_K = 5
FETCH_K = 20

RAG_CHAIN = None  # ì „ì—­ ì²´ì¸ ìºì‹±

# ==============================================================
# ìˆ˜ì¤€ë³„ ë‹µë³€ ê·œì¹™
# ==============================================================

GRADE_RULES = {
    "ì´ˆê¸‰": """
[ì´ˆê¸‰ì ë‹µë³€ ê·œì¹™]

ë‹¹ì‹ ì€ í”„ë¡œê·¸ë˜ë°/ë°ì´í„° ë¶„ì•¼ë¥¼ ì²˜ìŒ ë°°ìš°ëŠ” ì´ˆê¸‰ìë¥¼ ë•ëŠ” í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì„¤ëª…ì€ ë°˜ë“œì‹œ ì‰¬ìš´ í•œêµ­ì–´ë¡œ, ì§§ì€ ë¬¸ì¥ ìœ„ì£¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

--- (ìƒëµ: ë„ˆê°€ ì§ì ‘ ì‘ì„±í•œ ì´ˆê¸‰ í…œí”Œë¦¿ ë‚´ìš© ê·¸ëŒ€ë¡œ ìœ ì§€) ---
""",
    "ì¤‘ê¸‰": """
- ê°œë…ì˜ í•µì‹¬ ì •ì˜ë¥¼ ì •í™•í•˜ê²Œ ì œê³µ
- í•„ìš” ì‹œ ìš©ì–´ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ ë¶ˆí•„ìš”í•œ í™•ì¥ ê¸ˆì§€
- ì™œ ì´ëŸ° ê°œë…ì´ í•„ìš”í•œì§€ 1ë²ˆ ì„¤ëª…
- ì‹¤ë¬´ì—ì„œ í—·ê°ˆë¦¬ëŠ” í¬ì¸íŠ¸ë„ í•¨ê»˜ ì œê³µ
- ì¶œì²˜ ëª…ì‹œ
""",
    "ê³ ê¸‰": """
- ë‚´ë¶€ ë™ì‘ ì›ë¦¬ ì¤‘ì‹¬ ì„¤ëª…
- êµ¬ì¡°, ë©”ì»¤ë‹ˆì¦˜, ì„±ëŠ¥, ë©”ëª¨ë¦¬ ë“± ì‹¬í™” ë‚´ìš© í¬í•¨ ê°€ëŠ¥
- ë‹¤ë¥¸ ê¸°ìˆ ê³¼ ë¹„êµ ì„¤ëª… ê°€ëŠ¥
- ì¶œì²˜ ëª…ì‹œ
"""
}

# ==============================================================
# RAG ì²´ì¸ ì´ˆê¸°í™” + ìºì‹±
# ==============================================================

def initialize_rag_chain():
    print("\n[LOG] RAG ì²´ì¸ ì´ˆê¸°í™” ì‹œì‘")
    start = time.time()

    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name=COLLECTION
    )

    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": SEARCH_K, "fetch_k": FETCH_K}
    )

    template = """
ë‹¹ì‹ ì€ ë¶€íŠ¸ìº í”„ í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹µë³€ì€ ë°˜ë“œì‹œ ì œê³µëœ Context ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™”]
{history}

[í•™ìƒ ìˆ˜ì¤€]
{grade}

[ë‹µë³€ ê·œì¹™]
{grade_rules}

-------------------------
[Context]
{context}

[Question]
{question}
-------------------------
"""
    prompt = ChatPromptTemplate.from_template(template)
    model = ChatOpenAI(model=LLM_MODEL, temperature=0.2)

    rag_chain = (
        {
            "context": itemgetter("question") | retriever,
            "question": itemgetter("question"),
            "grade": itemgetter("grade"),
            "grade_rules": itemgetter("grade_rules"),
            "history": itemgetter("history"),
        }
        | prompt
        | model
        | StrOutputParser()
    )

    print(f"[Time] RAG ì²´ì¸ ì´ˆê¸°í™”: {time.time() - start:.3f}ì´ˆ")
    return rag_chain


def get_rag_chain():
    global RAG_CHAIN
    if RAG_CHAIN is None:
        RAG_CHAIN = initialize_rag_chain()
    return RAG_CHAIN

# ==============================================================
# History ìƒì„±
# ==============================================================

def build_history_text(history, max_turns=3):
    if not history:
        return ""
    recent = history[-max_turns:]
    text = ""
    for turn in recent:
        text += f"í•™ìƒ: {turn['question']}\n"
        text += f"AI: {turn['answer']}\n\n"
    return text

# ==============================================================
# ì§ˆë¬¸ ë¶„ë¦¬ (LLM í˜¸ì¶œ í¬í•¨ â†’ ì‹œê°„ ì¸¡ì •)
# ==============================================================

def split_questions(user_message: str):
    start = time.time()

    splitter = ChatOpenAI(model=LLM_MODEL, temperature=0)

    prompt = ChatPromptTemplate.from_template(
        """
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë³´ê³ , ë‹¤ë¥¸ ìš”ì²­/ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì•„ë˜ì²˜ëŸ¼ ë¶„ë¦¬í•˜ì„¸ìš”.
1. ì§ˆë¬¸1
2. ì§ˆë¬¸2

ì‚¬ìš©ì ì…ë ¥:
{message}
"""
    )

    chain = prompt | splitter | StrOutputParser()
    raw = chain.invoke({"message": user_message})

    # ê²°ê³¼ íŒŒì‹±
    questions = []
    for line in raw.splitlines():
        line = line.strip()
        if line and line[0].isdigit() and "." in line:
            _, q = line.split(".", 1)
            questions.append(q.strip())

    if not questions:
        questions = [user_message]

    print(f"[Time] ì§ˆë¬¸ ë¶„ë¦¬(split_questions): {time.time() - start:.3f}ì´ˆ")
    return questions

# ==============================================================
# ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (ì‹œê°„ ì¸¡ì • í¬í•¨)
# ==============================================================

def answer_single(question: str, grade: str, history: list):
    total_start = time.time()

    rag = get_rag_chain()
    history_text = build_history_text(history)

    # LLM í˜¸ì¶œ ì‹œê°„ ì¸¡ì •
    llm_start = time.time()
    answer_text = rag.invoke({
        "question": question,
        "grade": grade,
        "grade_rules": GRADE_RULES[grade],
        "history": history_text,
    })
    llm_end = time.time()

    print(f"[Time] LLM ë‹µë³€ ìƒì„±: {llm_end - llm_start:.3f}ì´ˆ")
    print(f"[Time] answer_single ì „ì²´ ì²˜ë¦¬: {time.time() - total_start:.3f}ì´ˆ")

    # history ì €ì¥
    history.append({"question": question, "answer": answer_text})

    return answer_text

# ==============================================================
# ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
# ==============================================================

def multi_answer(user_message: str, grade: str, history: list):
    total_start = time.time()

    questions = split_questions(user_message)

    # ì§ˆë¬¸ í•˜ë‚˜ë©´ ë‹¨ì¼ ì²˜ë¦¬
    if len(questions) == 1:
        return answer_single(questions[0], grade, history)

    outputs = []
    for idx, q in enumerate(questions, start=1):
        q_start = time.time()
        ans = answer_single(q, grade, history)
        outputs.append(f"### ì§ˆë¬¸ {idx}\n> {q}\n\n{ans}\n\n---")
        print(f"[Time] ì§ˆë¬¸ {idx} ì²˜ë¦¬ì‹œê°„: {time.time() - q_start:.3f}ì´ˆ")

    print(f"[Time] multi_answer ì „ì²´ ì²˜ë¦¬: {time.time() - total_start:.3f}ì´ˆ")
    return "\n".join(outputs)

# ==============================================================
# CLI í…ŒìŠ¤íŠ¸ ë£¨í”„
# ==============================================================

if __name__ == "__main__":
    print("\n=== RAG í•™ìŠµ ë„ìš°ë¯¸ (ì‹œê°„ì¸¡ì • í¬í•¨) ===\n")

    history = []

    while True:
        msg = input("\nğŸ“Œ ì§ˆë¬¸ ì…ë ¥(exit ì¢…ë£Œ): ")
        if msg.lower() == "exit":
            break

        grade = input("ğŸ’¡ ë‚œì´ë„(ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰): ").strip()

        print("\nâ³ ë‹µë³€ ìƒì„± ì¤‘...\n")

        result = multi_answer(msg, grade, history)

        print("\nğŸ§  ë‹µë³€:\n", result)
        print("\n====================================\n")
